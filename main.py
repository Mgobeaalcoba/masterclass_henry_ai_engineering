#!/usr/bin/env python
"""
Demostraci√≥n: LLM CON contexto usando RAG (Retrieval Augmented Generation)

Muestra c√≥mo darle contexto a GPT sobre documentaci√≥n privada usando LangChain + ChromaDB.
El modelo ahora puede responder sobre datos internos que no est√°n en su entrenamiento.
"""

import os
# Configurar tokenizers para evitar warnings de paralelismo despu√©s de fork
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# Configuraci√≥n
DOCUMENTO = "documentacion_tecnica.md"
CHROMA_DB_DIR = "./chroma_db"

def detectar_modelo():
    """Detecta qu√© modelo de OpenAI est√° disponible en tu cuenta."""
    for modelo in ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]:
        try:
            ChatOpenAI(model=modelo, temperature=0, max_tokens=5).invoke("test")
            return modelo
        except:
            continue
    return None

def configurar_rag():
    """
    Configura el sistema RAG completo.
    
    Pasos: Carga documento ‚Üí Divide en chunks ‚Üí Crea embeddings ‚Üí Almacena en ChromaDB ‚Üí 
    Configura cadena de pregunta-respuesta con contexto.
    
    Returns:
        tuple: (rag_chain, modelo_actual, retriever)
    """
    print("\nüîß Configurando RAG (Retrieval Augmented Generation)...\n")
    
    # 1. Cargar documento
    print("üìÑ Cargando documento:", DOCUMENTO)
    documentos = TextLoader(DOCUMENTO, encoding="utf-8").load()
    print(f"   ‚úÖ Documento cargado ({len(documentos)} archivo(s))\n")
    
    # 2. Dividir en fragmentos para mejor recuperaci√≥n
    print("‚úÇÔ∏è  Dividiendo documento en fragmentos...")
    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len).split_documents(documentos)
    print(f"   ‚úÖ {len(chunks)} fragmentos creados\n")
    
    # 3. Crear embeddings y base vectorial (HuggingFace es gratis)
    print("üß† Creando embeddings (HuggingFace - 100% GRATIS) y base vectorial...")
    print("   ‚è≥ Primera vez puede tomar un momento (descarga modelo ~400MB)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={'device': 'cpu'})
    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=CHROMA_DB_DIR)
    print("   ‚úÖ Base vectorial creada (sin costo!)\n")
    
    # 4. Detectar y configurar modelo de chat
    print("üîç Detectando modelo disponible...")
    modelo = detectar_modelo()
    if not modelo:
        raise Exception("No se encontr√≥ ning√∫n modelo disponible")
    print(f"   ‚úÖ Usando modelo: {modelo}\n")
    # Configuraci√≥n anti-alucinaci√≥n: temperatura muy baja para reducir creatividad y alucinaciones
    llm = ChatOpenAI(model=modelo, temperature=0.1, max_tokens=500)
    
    # 5. Crear retriever (busca los 3 fragmentos m√°s relevantes)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # 6. Crear prompt template para el contexto (m√°s estricto para evitar alucinaciones)
    template = """Eres un asistente √∫til y preciso que responde preguntas bas√°ndote √öNICAMENTE en la documentaci√≥n proporcionada.

Contexto de la documentaci√≥n:
{context}

Pregunta del usuario: {question}

INSTRUCCIONES IMPORTANTES:
- Responde SOLO usando la informaci√≥n que est√° en el contexto proporcionado
- Si la informaci√≥n NO est√° en el contexto, di claramente "No tengo informaci√≥n sobre esto en la documentaci√≥n" o "No s√© sobre..."
- NO inventes, NO supongas, NO uses conocimiento general fuera del contexto
- S√© preciso y directo. Si no sabes algo, dilo claramente."""
    prompt = ChatPromptTemplate.from_template(template)
    
    # 7. Funci√≥n para formatear documentos recuperados
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # 8. Crear cadena RAG (LangChain Expression Language)
    print("üîó Configurando cadena de pregunta-respuesta...")
    rag_chain = ({"context": retriever | format_docs, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
    print("   ‚úÖ Sistema RAG listo\n")
    
    return rag_chain, modelo, retriever

def main():
    """
    Funci√≥n principal: Configura RAG y ejecuta chat interactivo con contexto.
    
    Demuestra c√≥mo el modelo ahora S√ç puede responder sobre datos privados
    porque tiene acceso a la documentaci√≥n t√©cnica mediante RAG.
    """
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: Configura OPENAI_API_KEY en el archivo .env")
        return
    
    try:
        rag_chain, modelo, retriever = configurar_rag()
        
        print("="*70)
        print("  üí¨ Chat con RAG - El modelo AHORA tiene contexto")
        print("="*70)
        print(f"\nüí° El modelo tiene acceso a {DOCUMENTO}")
        print("üí° Escribe 'salir' para terminar\n")
        print("üéØ Prueba preguntando: ¬øC√≥mo instalo la librer√≠a HenryPy?\n")
        
        while True:
            pregunta = input("üßë T√ö: ").strip()
            
            if pregunta.lower() in ['salir', 'exit', 'quit']:
                print("\nüëã ¬°Hasta luego!\n")
                break
            
            if not pregunta:
                continue
            
            try:
                print(f"\n‚è≥ Buscando en documentaci√≥n y consultando {modelo}...\n")
                respuesta = rag_chain.invoke(pregunta)
                print(f"ü§ñ {modelo.upper()} (CON CONTEXTO): {respuesta}\n")
                print(f"üìö Fuentes: {len(retriever.invoke(pregunta))} fragmentos consultados")
                print("-" * 70 + "\n")
            except Exception as e:
                print(f"‚ùå Error: {e}\n")
    
    except Exception as e:
        print(f"‚ùå Error al configurar RAG: {e}\n")
        print("üí° Aseg√∫rate de que:")
        print(f"   - El archivo {DOCUMENTO} existe")
        print("   - Tu API Key de OpenAI es v√°lida")
        print("   - Tienes las dependencias instaladas (poetry install)\n")

if __name__ == "__main__":
    main()
