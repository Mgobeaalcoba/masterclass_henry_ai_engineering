<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>&#x1f3af; Prompt Engineering&colon; Generaci&oacute;n de Scripts RAG</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="-prompt-engineering-generaci√≥n-de-scripts-rag">üéØ Prompt Engineering: Generaci√≥n de Scripts RAG</h1>
<p>Este documento contiene ejemplos de <strong>prompt engineering</strong> para generar los tres scripts principales del proyecto usando un solo prompt cada uno.</p>
<hr>
<h2 id="-√≠ndice">üìã √çndice</h2>
<ol>
<li><a href="#1-script-sin-contexto-model_without_contextpy">Script sin Contexto</a></li>
<li><a href="#2-script-con-rag-mainpy">Script con RAG</a></li>
<li><a href="#3-script-h%C3%ADbrido-main_hybridpy">Script H√≠brido</a></li>
</ol>
<hr>
<h2 id="1-script-sin-contexto-model_without_contextpy">1. Script sin Contexto (<code>model_without_context.py</code>)</h2>
<h3 id="-objetivo">üéØ Objetivo</h3>
<p>Crear un script simple que demuestre c√≥mo un LLM <strong>NO puede responder</strong> sobre informaci√≥n privada porque no tiene acceso a documentaci√≥n interna.</p>
<h3 id="-prompt-completo">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `model_without_context.py` que demuestre la falta de contexto en LLMs.

OBJETIVO DEL SISTEMA:
El script debe demostrar que los modelos de lenguaje grandes (LLMs) NO pueden responder sobre informaci√≥n privada de empresas porque no tienen acceso a documentaci√≥n interna. El sistema debe implementar un chat interactivo simple con OpenAI GPT que muestre claramente esta limitaci√≥n educativa.

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- openai 2.8.0 o superior
- python-dotenv 1.2.1 o superior

REQUISITOS FUNCIONALES:

1. DETECCI√ìN AUTOM√ÅTICA DE MODELO:
   - El sistema debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible en la cuenta del usuario
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible y continuar con el siguiente
   - Debe informar al usuario qu√© modelo se est√° utilizando

2. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar mensajes informativos durante el proceso de consulta

3. CONFIGURACI√ìN ANTI-ALUCINACI√ìN:
   - El sistema debe configurar el modelo con temperatura muy baja (0.1) para minimizar creatividad y alucinaciones
   - Debe usar top_p de 0.9 para nucleus sampling m√°s restrictivo
   - Debe limitar las respuestas a m√°ximo 500 tokens
   - Debe incluir un prompt del sistema que instruya expl√≠citamente al modelo a decir &quot;No s√© sobre...&quot; cuando no tiene informaci√≥n, en lugar de inventar o suponer

4. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el chat
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

5. MANEJO DE ERRORES:
   - El sistema debe manejar errores de conexi√≥n, autenticaci√≥n y otros errores de API
   - Debe mostrar mensajes de error amigables al usuario sin exponer detalles t√©cnicos internos
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. ESTRUCTURA DEL ARCHIVO:
   - Debe incluir shebang para Python
   - Debe tener docstring principal que explique el prop√≥sito educativo del script
   - Debe organizarse en funciones claramente definidas

2. IMPORTS REQUERIDOS:
   - M√≥dulo os del sistema est√°ndar de Python
   - Clase OpenAI del paquete openai
   - Funci√≥n load_dotenv del paquete dotenv

3. CONFIGURACI√ìN DEL MODELO:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - top_p: 0.9 (nucleus sampling restrictivo)
   - System prompt: Debe instruir al modelo a ser honesto y decir &quot;No s√© sobre...&quot; cuando no tiene informaci√≥n

4. MENSAJES DE USUARIO:
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Debe mostrar claramente el nombre del modelo en uso
   - Debe incluir separadores visuales entre conversaciones
   - Mensajes deben estar en espa√±ol

5. DOCUMENTACI√ìN:
   - Todas las funciones deben tener docstrings en espa√±ol
   - Comentarios en espa√±ol cuando sean necesarios
   - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar claramente que el modelo NO conoce datos privados de empresas, permitiendo al usuario hacer preguntas interactivamente y observando c√≥mo el modelo responde honestamente cuando no tiene informaci√≥n, sin alucinar o inventar detalles.
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Especifica el objetivo educativo</strong>: &quot;demostrar la falta de contexto&quot;</li>
<li><strong>Lista requisitos funcionales</strong>: qu√© debe hacer el script</li>
<li><strong>Lista requisitos t√©cnicos</strong>: librer√≠as y configuraciones espec√≠ficas</li>
<li><strong>Define estructura</strong>: funciones esperadas y organizaci√≥n</li>
<li><strong>Incluye detalles espec√≠ficos</strong>: valores de temperatura, system prompt exacto</li>
</ul>
<hr>
<h2 id="2-script-con-rag-mainpy">2. Script con RAG (<code>main.py</code>)</h2>
<h3 id="-objetivo-1">üéØ Objetivo</h3>
<p>Crear un script que implemente RAG completo usando LangChain + ChromaDB para dar contexto al modelo sobre documentaci√≥n privada.</p>
<h3 id="-prompt-completo-1">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `main.py` que implemente un sistema RAG (Retrieval Augmented Generation) completo usando LangChain y ChromaDB.

OBJETIVO DEL SISTEMA:
El script debe demostrar c√≥mo darle contexto a GPT sobre documentaci√≥n privada usando LangChain + ChromaDB. El sistema debe permitir que el modelo responda sobre datos internos que no est√°n en su entrenamiento mediante la implementaci√≥n de un sistema RAG completo que carga documentaci√≥n t√©cnica, la procesa, y permite hacer preguntas interactivas que se responden usando el contexto de la documentaci√≥n.

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- langchain 1.0.7 o superior
- langchain-openai 1.0.3 o superior
- langchain-community 0.4.1 o superior
- langchain-huggingface 1.0.0 o superior (CR√çTICO: usar esta versi√≥n para evitar deprecaciones de HuggingFaceEmbeddings)
- chromadb 1.3.4 o superior
- python-dotenv 1.2.1 o superior
- openai 2.8.0 o superior
- sentence-transformers 5.1.2 o superior

REQUISITOS FUNCIONALES:

1. CARGA Y PROCESAMIENTO DE DOCUMENTACI√ìN:
   - El sistema debe cargar un documento markdown desde el archivo &quot;documentacion_tecnica.md&quot;
   - Debe dividir el documento en fragmentos (chunks) de tama√±o 500 caracteres con solapamiento de 50 caracteres
   - Debe crear embeddings vectoriales usando el modelo HuggingFace &quot;sentence-transformers/all-MiniLM-L6-v2&quot;
   - Debe almacenar los vectores en una base de datos vectorial ChromaDB persistente en el directorio &quot;./chroma_db&quot;
   - Debe mostrar mensajes informativos durante cada paso del proceso de configuraci√≥n

2. SISTEMA DE RECUPERACI√ìN (RETRIEVER):
   - El sistema debe configurar un retriever que busque los 3 fragmentos m√°s relevantes para cada pregunta
   - Debe usar b√∫squeda por similitud sem√°ntica basada en embeddings
   - Debe informar al usuario cu√°ntos fragmentos se consultaron para cada respuesta

3. CADENA RAG CON LANGCHAIN EXPRESSION LANGUAGE (LCEL):
   - El sistema debe implementar una cadena RAG usando LCEL (LangChain Expression Language)
   - La cadena debe combinar: recuperaci√≥n de contexto ‚Üí formateo de documentos ‚Üí prompt template ‚Üí modelo LLM ‚Üí parser de salida
   - Debe usar el operador pipe de Python para componer la cadena
   - CR√çTICO: Debe usar la API moderna de LangChain (v1.0+), NO m√©todos deprecados

4. DETECCI√ìN AUTOM√ÅTICA DE MODELO:
   - El sistema debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible

5. CONFIGURACI√ìN ANTI-ALUCINACI√ìN:
   - El sistema debe configurar el modelo con temperatura 0.1 para minimizar alucinaciones
   - Debe limitar las respuestas a m√°ximo 500 tokens
   - Debe incluir un prompt template estricto que instruya al modelo a usar SOLO el contexto proporcionado
   - El prompt debe indicar expl√≠citamente que si la informaci√≥n NO est√° en el contexto, debe decir &quot;No tengo informaci√≥n sobre esto en la documentaci√≥n&quot;

6. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar qu√© fragmentos de documentaci√≥n se consultaron para cada respuesta

7. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el sistema RAG
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

8. MANEJO DE ERRORES:
   - El sistema debe manejar errores de carga de documento, creaci√≥n de embeddings, y consultas al modelo
   - Debe mostrar mensajes de error amigables con sugerencias de soluci√≥n
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. CONFIGURACI√ìN DE TOKENIZERS:
   - El sistema debe configurar la variable de entorno TOKENIZERS_PARALLELISM=&quot;false&quot; antes de importar cualquier m√≥dulo de HuggingFace
   - Esto evita warnings de paralelismo despu√©s de fork

2. IMPORTS REQUERIDOS:
   - M√≥dulo os del sistema est√°ndar de Python
   - Funci√≥n load_dotenv del paquete dotenv
   - Clase ChatOpenAI del paquete langchain_openai
   - Clase HuggingFaceEmbeddings del paquete langchain_huggingface (CR√çTICO: NO usar langchain_community.embeddings que est√° deprecado)
   - Clase TextLoader del paquete langchain_community.document_loaders
   - Clase Chroma del paquete langchain_community.vectorstores
   - Clase RecursiveCharacterTextSplitter del paquete langchain_text_splitters
   - Clase ChatPromptTemplate del paquete langchain_core.prompts
   - Clase RunnablePassthrough del paquete langchain_core.runnables
   - Clase StrOutputParser del paquete langchain_core.output_parsers

3. CONFIGURACI√ìN DE CHUNKS:
   - Tama√±o de chunk: 500 caracteres
   - Solapamiento entre chunks: 50 caracteres
   - Funci√≥n de longitud: len (contar caracteres)

4. CONFIGURACI√ìN DE EMBEDDINGS:
   - Modelo: sentence-transformers/all-MiniLM-L6-v2
   - Dispositivo: CPU
   - Proveedor: HuggingFace (gratuito, sin costo)

5. CONFIGURACI√ìN DEL MODELO LLM:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - Modelo: Detectado autom√°ticamente de la lista disponible

6. PROMPT TEMPLATE:
   - Debe instruir al modelo a responder √öNICAMENTE usando la documentaci√≥n proporcionada
   - Debe incluir instrucciones expl√≠citas para evitar alucinaciones
   - Debe indicar que si no hay informaci√≥n en el contexto, debe decir claramente &quot;No tengo informaci√≥n sobre esto en la documentaci√≥n&quot;

7. ESTRUCTURA DE ARCHIVOS:
   - Archivo de documentaci√≥n: &quot;documentacion_tecnica.md&quot;
   - Directorio de base vectorial: &quot;./chroma_db&quot;
   - Archivo de configuraci√≥n: &quot;.env&quot; (para API Key)

8. API MODERNA DE LANGCHAIN:
   - CR√çTICO: Usar langchain_huggingface para HuggingFaceEmbeddings (NO langchain_community.embeddings)
   - CR√çTICO: Usar retriever.invoke() para buscar documentos (NO m√©todos deprecados como get_relevant_documents)
   - CR√çTICO: Usar LangChain Expression Language (LCEL) con operador pipe para construir cadenas

9. MENSAJES DE USUARIO:
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Debe mostrar claramente el nombre del modelo en uso
   - Debe incluir separadores visuales entre conversaciones
   - Mensajes deben estar en espa√±ol

10. DOCUMENTACI√ìN:
    - Todas las funciones deben tener docstrings en espa√±ol
    - Comentarios en espa√±ol cuando sean necesarios
    - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar claramente c√≥mo el modelo AHORA S√ç puede responder sobre datos privados usando RAG, permitiendo al usuario hacer preguntas interactivamente y observando c√≥mo el modelo responde usando el contexto de la documentaci√≥n t√©cnica, sin alucinar informaci√≥n que no est√° en la documentaci√≥n.

FUNCIONALIDADES ESPERADAS:

1. FUNCI√ìN DE DETECCI√ìN DE MODELO:
   - El sistema debe implementar una funci√≥n que detecte autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible y continuar con el siguiente
   - Debe retornar el nombre del modelo disponible o None si ninguno est√° disponible

2. FUNCI√ìN DE CONFIGURACI√ìN RAG:
   - El sistema debe implementar una funci√≥n que configure el sistema RAG completo
   - La funci√≥n debe realizar los siguientes pasos en orden:
     a) Cargar documento markdown desde archivo
     b) Dividir documento en fragmentos con tama√±o y solapamiento especificados
     c) Crear embeddings vectoriales usando HuggingFace
     d) Almacenar vectores en ChromaDB persistente
     e) Detectar y configurar modelo LLM disponible
     f) Crear retriever que busque fragmentos relevantes
     g) Crear prompt template con instrucciones estrictas
     h) Construir cadena RAG usando LangChain Expression Language
   - La funci√≥n debe mostrar mensajes informativos durante cada paso
   - La funci√≥n debe retornar una tupla con (rag_chain, modelo, retriever)

3. FUNCI√ìN PRINCIPAL:
   - El sistema debe implementar una funci√≥n principal que orqueste todo el flujo
   - Debe validar la existencia de la API Key antes de iniciar
   - Debe configurar el sistema RAG llamando a la funci√≥n de configuraci√≥n
   - Debe mostrar mensajes de bienvenida e instrucciones al usuario
   - Debe implementar un loop interactivo que permita hacer preguntas continuamente
   - Debe procesar cada pregunta usando la cadena RAG
   - Debe mostrar la respuesta junto con informaci√≥n sobre los fragmentos consultados
   - Debe manejar errores de forma amigable y permitir continuar despu√©s de errores
   - Debe reconocer comandos de salida y terminar el programa apropiadamente

4. MENSAJES Y FEEDBACK AL USUARIO:
   - El sistema debe mostrar mensajes informativos durante la configuraci√≥n del RAG
   - Debe mostrar mensajes de bienvenida con informaci√≥n sobre el sistema
   - Debe mostrar mensajes durante el procesamiento de cada pregunta
   - Debe mostrar claramente qu√© fragmentos de documentaci√≥n se consultaron
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Mensajes deben estar en espa√±ol

5. ESTRUCTURA Y ORGANIZACI√ìN:
   - El c√≥digo debe estar organizado en funciones claramente definidas
   - Cada funci√≥n debe tener docstrings en espa√±ol explicando su prop√≥sito
   - El c√≥digo debe seguir convenciones PEP 8
   - Debe incluir punto de entrada est√°ndar de Python
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt-1">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Especifica arquitectura completa</strong>: todos los pasos del pipeline RAG</li>
<li><strong>Detalla configuraciones</strong>: valores espec√≠ficos de chunk_size, modelo de embeddings, etc.</li>
<li><strong>Incluye estructura LCEL</strong>: muestra exactamente c√≥mo construir la cadena</li>
<li><strong>Define prompt template</strong>: texto exacto del prompt para evitar alucinaciones</li>
<li><strong>Especifica retornos</strong>: qu√© debe retornar cada funci√≥n</li>
</ul>
<hr>
<h2 id="3-script-h√≠brido-main_hybridpy">3. Script H√≠brido (<code>main_hybrid.py</code>)</h2>
<h3 id="-objetivo-2">üéØ Objetivo</h3>
<p>Crear un script que combine RAG con conocimiento del modelo: primero busca en documentaci√≥n, si no encuentra usa conocimiento propio.</p>
<h3 id="-prompt-completo-2">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `main_hybrid.py` que implemente un sistema H√çBRIDO que combine RAG con conocimiento del entrenamiento del modelo.

OBJETIVO DEL SISTEMA:
El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo. El sistema debe implementar una estrategia h√≠brida que primero busca informaci√≥n en documentaci√≥n privada usando RAG, y si no encuentra informaci√≥n relevante, usa el conocimiento del entrenamiento del modelo. Si el modelo tampoco sabe, debe decir expl√≠citamente &quot;No s√©&quot;. El sistema debe mostrar claramente qu√© fuente utiliz√≥ para responder cada pregunta (documentaci√≥n o conocimiento propio).

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- langchain 1.0.7 o superior
- langchain-openai 1.0.3 o superior
- langchain-community 0.4.1 o superior
- langchain-huggingface 1.0.0 o superior (CR√çTICO: usar esta versi√≥n para evitar deprecaciones de HuggingFaceEmbeddings)
- chromadb 1.3.4 o superior
- python-dotenv 1.2.1 o superior
- openai 2.8.0 o superior
- sentence-transformers 5.1.2 o superior

REQUISITOS FUNCIONALES:

1. CONFIGURACI√ìN DEL SISTEMA H√çBRIDO:
   - El sistema debe configurar un sistema RAG completo id√©ntico al script main.py (carga de documento, chunks, embeddings, vectorstore)
   - Debe configurar un retriever que busque los 3 fragmentos m√°s relevantes
   - Debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe mostrar mensajes informativos durante la configuraci√≥n indicando que es un sistema h√≠brido

2. EVALUACI√ìN DE RELEVANCIA DE DOCUMENTOS:
   - El sistema debe implementar una funci√≥n que eval√∫e si los documentos recuperados son relevantes para la pregunta del usuario
   - La evaluaci√≥n debe considerar:
     - Si los documentos est√°n vac√≠os o no existen ‚Üí No relevante
     - Si el contenido total es menor a 50 caracteres ‚Üí No relevante
     - Si el contenido total es mayor a 200 caracteres ‚Üí Generalmente relevante (confiar en el retriever)
     - Si hay coincidencias de palabras clave importantes (palabras de m√°s de 3 caracteres) entre la pregunta y el contenido ‚Üí Relevante
     - Si el contenido es entre 50-200 caracteres y hay coincidencias de palabras clave (palabras de m√°s de 2 caracteres) ‚Üí Relevante
   - Debe filtrar stopwords en espa√±ol antes de evaluar coincidencias
   - Debe retornar True si hay informaci√≥n relevante, False si no

3. ESTRATEGIA DE RESPUESTA H√çBRIDA:
   - El sistema debe implementar la siguiente estrategia de decisi√≥n:
     a) DETECCI√ìN DE SOLICITUD EXPL√çCITA: Si el usuario pide expl√≠citamente usar conocimiento fuera de las fuentes (palabras clave: 'fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n', 'conocimiento propio', 'entrenamiento'), debe usar directamente el conocimiento del modelo sin buscar en documentaci√≥n
     b) B√öSQUEDA EN DOCUMENTACI√ìN: Si no hay solicitud expl√≠cita, debe buscar documentos relevantes usando el retriever
     c) EVALUACI√ìN DE RELEVANCIA: Debe evaluar si los documentos encontrados son relevantes usando la funci√≥n de evaluaci√≥n
     d) RESPUESTA CON RAG: Si hay informaci√≥n relevante, debe responder usando RAG con la documentaci√≥n
     e) FALLBACK RAG DIRECTO: Si RAG responde con informaci√≥n insuficiente (menos de 50 caracteres y dice &quot;no tengo informaci√≥n&quot; o &quot;no s√©&quot;), debe intentar una vez m√°s con un prompt m√°s directo usando los mismos documentos
     f) RESPUESTA CON CONOCIMIENTO PROPIO: Si no hay informaci√≥n relevante en la documentaci√≥n, debe responder usando el conocimiento del entrenamiento del modelo
   - El sistema debe retornar tanto la respuesta como la fuente utilizada (&quot;documentaci√≥n&quot; o &quot;conocimiento del modelo&quot;)

4. M√öLTIPLES MODOS DE RESPUESTA:
   - El sistema debe implementar tres funciones de respuesta:
     a) responder_con_rag: Usa RAG cuando hay informaci√≥n en la documentaci√≥n, con prompt estricto que instruye usar SOLO el contexto proporcionado
     b) responder_con_rag_directo: Usa RAG con documentos ya recuperados como fallback cuando el prompt normal falla, con un prompt m√°s directo y simple
     c) responder_con_conocimiento_propio: Usa el conocimiento del entrenamiento del modelo cuando no hay informaci√≥n relevante en la documentaci√≥n, con prompt que instruye ser honesto y decir &quot;No s√©&quot; cuando no sabe

5. INDICADORES VISUALES DE FUENTE:
   - El sistema debe mostrar claramente qu√© fuente se utiliz√≥ para responder cada pregunta
   - Debe usar emojis diferentes: üìö para documentaci√≥n, üß† para conocimiento propio
   - Debe mostrar el texto &quot;DOCUMENTACI√ìN&quot; o &quot;CONOCIMIENTO PROPIO&quot; junto al emoji
   - Si us√≥ documentaci√≥n, debe mostrar cu√°ntos fragmentos se consultaron

6. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar mensajes informativos sobre la estrategia h√≠brida al inicio

7. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el sistema
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

8. MANEJO DE ERRORES:
   - El sistema debe manejar errores de configuraci√≥n, carga de documento, y consultas al modelo
   - Debe mostrar mensajes de error amigables con sugerencias de soluci√≥n
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. CONFIGURACI√ìN DE TOKENIZERS:
   - El sistema debe configurar la variable de entorno TOKENIZERS_PARALLELISM=&quot;false&quot; antes de importar cualquier m√≥dulo de HuggingFace
   - Esto evita warnings de paralelismo despu√©s de fork

2. IMPORTS REQUERIDOS:
   - Mismos imports que main.py: os, dotenv, langchain_openai, langchain_huggingface (CR√çTICO: NO usar langchain_community.embeddings), langchain_community.document_loaders, langchain_community.vectorstores, langchain_text_splitters, langchain_core.prompts, langchain_core.runnables, langchain_core.output_parsers

3. CONSTANTES DE CONFIGURACI√ìN:
   - DOCUMENTO: &quot;documentacion_tecnica.md&quot;
   - CHROMA_DB_DIR: &quot;./chroma_db&quot;

4. CONFIGURACI√ìN RAG (igual que main.py):
   - Tama√±o de chunk: 500 caracteres
   - Solapamiento: 50 caracteres
   - Modelo de embeddings: sentence-transformers/all-MiniLM-L6-v2
   - Dispositivo: CPU
   - Retriever: busca 3 fragmentos m√°s relevantes

5. CONFIGURACI√ìN DEL MODELO LLM:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - Modelo: Detectado autom√°ticamente

6. EVALUACI√ìN DE RELEVANCIA - STOPWORDS EN ESPA√ëOL:
   - El sistema debe filtrar estos stopwords exactos antes de evaluar coincidencias: 'qu√©', 'c√≥mo', 'cu√°ndo', 'd√≥nde', 'por', 'para', 'con', 'de', 'la', 'el', 'un', 'una', 'es', 'son', 'est√°', 'est√°n', 'sobre', 'los', 'las', 'y', 'o', 'pero', 'si', 'no', 'en', 'a', 'que', 'se', 'le', 'te', 'me', 'nos', 'les', 'puedes', 'puede', 'puedo', 'pueden', 'pueda', 'puedan', 'sirve', 'sirven', 'consultar', 'fuera', 'fuentes', 'tus', 'sus', 'mis', 'nuestros', 'vuestros', 'trata', 'como', 'instala', 'instalar'

7. PROMPT TEMPLATES:
   - Template RAG: Debe instruir usar √öNICAMENTE la documentaci√≥n proporcionada, ser exhaustivo si hay informaci√≥n, y decir &quot;No tengo informaci√≥n sobre esto en la documentaci√≥n&quot; si no hay informaci√≥n
   - Template RAG Directo: Debe ser m√°s simple y directo, instruyendo usar SOLO la informaci√≥n de la documentaci√≥n
   - Template Conocimiento Propio: Debe instruir usar conocimiento general, ser honesto, y decir &quot;No s√© sobre...&quot; cuando no sabe

8. DETECCI√ìN DE SOLICITUD EXPL√çCITA:
   - El sistema debe detectar si el usuario pide expl√≠citamente usar conocimiento fuera de fuentes
   - Palabras clave a detectar: 'fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n', 'conocimiento propio', 'entrenamiento'
   - Si detecta estas palabras, debe limpiar la pregunta removiendo estas palabras y usar conocimiento propio directamente

9. API MODERNA DE LANGCHAIN:
   - CR√çTICO: Usar langchain_huggingface para HuggingFaceEmbeddings (NO langchain_community.embeddings)
   - CR√çTICO: Usar retriever.invoke() para buscar documentos (NO m√©todos deprecados)
   - CR√çTICO: Usar LangChain Expression Language (LCEL) con operador pipe para construir cadenas

10. MENSAJES DE USUARIO:
    - Todos los mensajes deben incluir emojis apropiados
    - Debe mostrar claramente qu√© fuente se us√≥ (üìö DOCUMENTACI√ìN o üß† CONOCIMIENTO PROPIO)
    - Debe incluir separadores visuales entre conversaciones
    - Mensajes deben estar en espa√±ol

11. DOCUMENTACI√ìN:
    - Todas las funciones deben tener docstrings en espa√±ol explicando su prop√≥sito
    - Comentarios en espa√±ol cuando sean necesarios
    - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo. El sistema debe decidir inteligentemente qu√© fuente usar para responder cada pregunta, mostrando claramente al usuario si la respuesta viene de la documentaci√≥n o del conocimiento propio del modelo. El usuario debe poder hacer preguntas interactivamente y observar c√≥mo el sistema eval√∫a la relevancia y selecciona la mejor fuente de informaci√≥n.
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt-2">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Define estrategia completa</strong>: flujo de decisi√≥n paso a paso</li>
<li><strong>Especifica m√∫ltiples funciones</strong>: cada una con prop√≥sito claro</li>
<li><strong>Detalla l√≥gica de evaluaci√≥n</strong>: c√≥mo determinar relevancia</li>
<li><strong>Incluye casos especiales</strong>: detecci√≥n de solicitud expl√≠cita, fallback</li>
<li><strong>Define estructura completa</strong>: todas las funciones necesarias</li>
</ul>
<hr>
<h2 id="-mejores-pr√°cticas-de-prompt-engineering">üéì Mejores Pr√°cticas de Prompt Engineering</h2>
<h3 id="1-estructura-clara">1. <strong>Estructura Clara</strong></h3>
<ul>
<li>Separar en secciones: REQUISITOS FUNCIONALES, REQUISITOS T√âCNICOS, ESTRUCTURA ESPERADA</li>
<li>Usar listas numeradas o con vi√±etas para mejor legibilidad</li>
</ul>
<h3 id="2-especificidad">2. <strong>Especificidad</strong></h3>
<ul>
<li>Incluir valores exactos: <code>temperature=0.1</code>, <code>chunk_size=500</code></li>
<li>Mencionar librer√≠as espec√≠ficas: <code>langchain_openai</code>, <code>HuggingFaceEmbeddings</code></li>
<li>Definir nombres de funciones y variables esperadas</li>
</ul>
<h3 id="3-contexto-y-objetivo">3. <strong>Contexto y Objetivo</strong></h3>
<ul>
<li>Empezar con el objetivo educativo o funcional</li>
<li>Explicar el &quot;por qu√©&quot; adem√°s del &quot;qu√©&quot;</li>
</ul>
<h3 id="4-descripciones-detalladas-sin-c√≥digo">4. <strong>Descripciones Detalladas Sin C√≥digo</strong></h3>
<ul>
<li>Describir exactamente qu√© debe hacer cada funci√≥n sin mostrar c√≥digo</li>
<li>Especificar valores exactos, par√°metros, y estructuras de datos esperadas</li>
<li>Usar lenguaje descriptivo que permita generar c√≥digo preciso</li>
</ul>
<h3 id="5-restricciones-y-validaciones">5. <strong>Restricciones y Validaciones</strong></h3>
<ul>
<li>Especificar qu√© NO debe hacer (evitar alucinaciones)</li>
<li>Definir manejo de errores esperado</li>
</ul>
<h3 id="6-documentaci√≥n-esperada">6. <strong>Documentaci√≥n Esperada</strong></h3>
<ul>
<li>Solicitar docstrings en espa√±ol</li>
<li>Pedir mensajes informativos para UX</li>
</ul>
<hr>
<h2 id="-comparaci√≥n-de-prompts">üìä Comparaci√≥n de Prompts</h2>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>Script Sin Contexto</th>
<th>Script con RAG</th>
<th>Script H√≠brido</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Complejidad</strong></td>
<td>Baja</td>
<td>Media</td>
<td>Alta</td>
</tr>
<tr>
<td><strong>Librer√≠as</strong></td>
<td>2 (openai, dotenv)</td>
<td>7+ (LangChain completo)</td>
<td>7+ (LangChain completo)</td>
</tr>
<tr>
<td><strong>Funciones</strong></td>
<td>2</td>
<td>3</td>
<td>8+</td>
</tr>
<tr>
<td><strong>L√≥gica de Decisi√≥n</strong></td>
<td>Ninguna</td>
<td>Simple (solo RAG)</td>
<td>Compleja (RAG + evaluaci√≥n + fallback)</td>
</tr>
<tr>
<td><strong>Prompt Length</strong></td>
<td>~300 palabras</td>
<td>~500 palabras</td>
<td>~800 palabras</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="-uso-de-los-prompts">üöÄ Uso de los Prompts</h2>
<h3 id="opci√≥n-1-copiar-y-pegar-directo">Opci√≥n 1: Copiar y Pegar Directo</h3>
<p>Copia el prompt completo en tu herramienta de IA favorita (Claude, GPT-4, etc.) y solicita la generaci√≥n del c√≥digo.</p>
<h3 id="opci√≥n-2-adaptaci√≥n-incremental">Opci√≥n 2: Adaptaci√≥n Incremental</h3>
<ol>
<li>Empieza con el prompt base</li>
<li>Agrega requisitos espec√≠ficos de tu proyecto</li>
<li>Refina seg√∫n necesidades</li>
</ol>
<h3 id="opci√≥n-3-prompt-modular">Opci√≥n 3: Prompt Modular</h3>
<p>Divide el prompt en secciones y genera cada parte por separado, luego combina.</p>
<hr>
<h2 id="Ô∏è-errores-comunes-y-c√≥mo-evitarlos">‚ö†Ô∏è Errores Comunes y C√≥mo Evitarlos</h2>
<h3 id="1-error-vectorstoreretriever-object-has-no-attribute-get_relevant_documents">1. <strong>Error: <code>'VectorStoreRetriever' object has no attribute 'get_relevant_documents'</code></strong></h3>
<p><strong>Causa</strong>: Uso de m√©todos deprecados de LangChain.</p>
<p><strong>Soluci√≥n en el prompt</strong>:</p>
<ul>
<li>Especificar claramente en el prompt que se debe usar el m√©todo invoke del retriever pasando la pregunta como argumento para buscar documentos</li>
<li>NO mencionar en el prompt m√©todos como get_relevant_documents o retrieve que est√°n deprecados</li>
<li>Especificar que en LCEL, el retriever se usa directamente con el operador pipe, donde el operador pipe ejecuta autom√°ticamente invoke</li>
<li>Describir que para obtener fragmentos consultados se debe llamar al m√©todo invoke del retriever pasando la pregunta, lo cual retorna una lista de documentos</li>
</ul>
<h3 id="2-warning-huggingfaceembeddings-was-deprecated">2. <strong>Warning: <code>HuggingFaceEmbeddings</code> was deprecated</strong></h3>
<p><strong>Causa</strong>: Uso de import deprecado desde <code>langchain_community.embeddings</code>.</p>
<p><strong>Soluci√≥n en el prompt</strong>:</p>
<ul>
<li>Especificar expl√≠citamente en el prompt que se debe importar HuggingFaceEmbeddings desde el paquete langchain_huggingface</li>
<li>NO mencionar en el prompt importar desde langchain_community.embeddings que est√° deprecado</li>
<li>Incluir en las dependencias la librer√≠a langchain-huggingface versi√≥n 1.0.0 o superior</li>
<li>Agregar una nota cr√≠tica en el prompt indicando que usar langchain_community.embeddings est√° deprecado y causar√° warnings</li>
</ul>
<h3 id="3-dependencia-faltante-langchain-huggingface">3. <strong>Dependencia faltante: <code>langchain-huggingface</code></strong></h3>
<p><strong>Soluci√≥n en el prompt</strong>:</p>
<ul>
<li>Incluir en la lista de dependencias: &quot;langchain-huggingface (requerida para HuggingFaceEmbeddings)&quot;</li>
<li>Mencionar que debe instalarse: <code>poetry add langchain-huggingface</code> o <code>pip install langchain-huggingface</code></li>
</ul>
<h3 id="4-timeout-al-descargar-modelos-de-huggingface">4. <strong>Timeout al descargar modelos de HuggingFace</strong></h3>
<p><strong>Soluci√≥n en el prompt</strong>:</p>
<ul>
<li>Mencionar que la primera descarga puede tomar tiempo</li>
<li>Sugerir manejo de errores con mensajes informativos</li>
<li>Opcional: mencionar que el modelo se cachea localmente despu√©s de la primera descarga</li>
</ul>
<hr>
<h2 id="-tips-adicionales">üí° Tips Adicionales</h2>
<ol>
<li><strong>Iteraci√≥n</strong>: Los prompts pueden necesitar refinamiento. Prueba y ajusta.</li>
<li><strong>Especificidad</strong>: Mientras m√°s espec√≠fico, mejor resultado. Incluye valores exactos.</li>
<li><strong>Ejemplos</strong>: Si tienes c√≥digo de referencia, incl√∫yelo en el prompt.</li>
<li><strong>Validaci√≥n</strong>: Siempre prueba el c√≥digo generado antes de usarlo en producci√≥n.</li>
<li><strong>Documentaci√≥n</strong>: Solicita expl√≠citamente documentaci√≥n en espa√±ol si la necesitas.</li>
<li><strong>API Moderna</strong>: Siempre especifica usar la API moderna de LangChain (v1.0+), NO m√©todos deprecados.</li>
<li><strong>Dependencias</strong>: Lista todas las dependencias necesarias, incluyendo <code>langchain-huggingface</code>.</li>
</ol>
<hr>
<h2 id="-dependencias-requeridas">üì¶ Dependencias Requeridas</h2>
<p>Para que los scripts generados funcionen correctamente, aseg√∫rate de tener estas dependencias en <code>pyproject.toml</code>:</p>
<pre><code class="language-toml"><span class="hljs-section">[tool.poetry.dependencies]</span>
<span class="hljs-attr">python</span> = <span class="hljs-string">&quot;^3.13&quot;</span>
<span class="hljs-attr">langchain</span> = <span class="hljs-string">&quot;^1.0.7&quot;</span>
<span class="hljs-attr">langchain-openai</span> = <span class="hljs-string">&quot;^1.0.3&quot;</span>
<span class="hljs-attr">langchain-community</span> = <span class="hljs-string">&quot;^0.4.1&quot;</span>
<span class="hljs-attr">langchain-huggingface</span> = <span class="hljs-string">&quot;^1.0.0&quot;</span>  <span class="hljs-comment"># ‚ö†Ô∏è REQUERIDA para HuggingFaceEmbeddings</span>
<span class="hljs-attr">chromadb</span> = <span class="hljs-string">&quot;^1.3.4&quot;</span>
<span class="hljs-attr">python-dotenv</span> = <span class="hljs-string">&quot;^1.2.1&quot;</span>
<span class="hljs-attr">openai</span> = <span class="hljs-string">&quot;^2.8.0&quot;</span>
<span class="hljs-attr">sentence-transformers</span> = <span class="hljs-string">&quot;^5.1.2&quot;</span>
</code></pre>
<p><strong>Instalaci√≥n</strong>:</p>
<pre><code class="language-bash">poetry add langchain-huggingface
<span class="hljs-comment"># o</span>
pip install langchain-huggingface
</code></pre>
<hr>
<h2 id="-notas-finales">üìù Notas Finales</h2>
<p>Estos prompts est√°n dise√±ados para generar c√≥digo funcional y bien documentado. Sin embargo, siempre:</p>
<ul>
<li>‚úÖ Revisa el c√≥digo generado</li>
<li>‚úÖ Prueba la funcionalidad</li>
<li>‚úÖ Ajusta seg√∫n tus necesidades espec√≠ficas</li>
<li>‚úÖ Valida dependencias y configuraciones</li>
<li>‚úÖ Verifica que uses la API moderna de LangChain (v1.0+)</li>
<li>‚úÖ Aseg√∫rate de tener <code>langchain-huggingface</code> instalado</li>
</ul>
<p>Los prompts pueden adaptarse para otros frameworks o lenguajes cambiando las librer√≠as y estructuras mencionadas.</p>

            
            
        </body>
        </html>