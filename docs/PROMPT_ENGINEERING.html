<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>&#x1f3af; Prompt Engineering&colon; Generaci&oacute;n de Scripts RAG</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="-prompt-engineering-generaci√≥n-de-scripts-rag">üéØ Prompt Engineering: Generaci√≥n de Scripts RAG</h1>
<p>Este documento contiene ejemplos de <strong>prompt engineering</strong> para generar los tres scripts principales del proyecto usando un solo prompt cada uno.</p>
<hr>
<h2 id="-√≠ndice">üìã √çndice</h2>
<ol>
<li><a href="#1-script-sin-contexto-model_without_contextpy">Script sin Contexto</a></li>
<li><a href="#2-script-con-rag-mainpy">Script con RAG</a></li>
<li><a href="#3-script-h%C3%ADbrido-main_hybridpy">Script H√≠brido</a></li>
</ol>
<hr>
<h2 id="1-script-sin-contexto-model_without_contextpy">1. Script sin Contexto (<code>model_without_context.py</code>)</h2>
<h3 id="-objetivo">üéØ Objetivo</h3>
<p>Crear un script simple que demuestre c√≥mo un LLM <strong>NO puede responder</strong> sobre informaci√≥n privada porque no tiene acceso a documentaci√≥n interna.</p>
<h3 id="-prompt-completo">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `model_without_context.py` que demuestre la falta de contexto en LLMs.

REQUISITOS FUNCIONALES:
1. Chat interactivo con OpenAI GPT usando la API oficial
2. Detecci√≥n autom√°tica del modelo disponible (gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)
3. Configuraci√≥n anti-alucinaci√≥n:
   - Temperature: 0.1 (muy baja)
   - top_p: 0.9
   - System prompt que instruya al modelo a decir &quot;No s√© sobre...&quot; cuando no tiene informaci√≥n
4. Loop de conversaci√≥n interactivo con comando 'salir' para terminar
5. Manejo de errores y validaci√≥n de API Key desde archivo .env

REQUISITOS T√âCNICOS:
- Usar `openai` (cliente oficial) y `python-dotenv`
- Funci√≥n `detectar_modelo(client)` que pruebe modelos en orden de preferencia
- Funci√≥n `main()` con loop interactivo
- System prompt que evite alucinaciones: &quot;Eres un asistente honesto y directo. Si no conoces o no tienes informaci√≥n sobre algo, debes decir claramente 'No s√© sobre...' o 'No tengo informaci√≥n sobre...' en lugar de inventar o suponer.&quot;
- Documentaci√≥n en espa√±ol en docstrings
- Mensajes informativos con emojis para mejor UX

ESTRUCTURA ESPERADA:
- Imports: os, OpenAI, load_dotenv
- Funci√≥n detectar_modelo(client) ‚Üí retorna modelo disponible o None
- Funci√≥n main() ‚Üí configuraci√≥n, loop interactivo, manejo de errores
- Punto de entrada: if __name__ == &quot;__main__&quot;

El script debe ser simple, directo y demostrar claramente que el modelo NO conoce datos privados.
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Especifica el objetivo educativo</strong>: &quot;demostrar la falta de contexto&quot;</li>
<li><strong>Lista requisitos funcionales</strong>: qu√© debe hacer el script</li>
<li><strong>Lista requisitos t√©cnicos</strong>: librer√≠as y configuraciones espec√≠ficas</li>
<li><strong>Define estructura</strong>: funciones esperadas y organizaci√≥n</li>
<li><strong>Incluye detalles espec√≠ficos</strong>: valores de temperatura, system prompt exacto</li>
</ul>
<hr>
<h2 id="2-script-con-rag-mainpy">2. Script con RAG (<code>main.py</code>)</h2>
<h3 id="-objetivo-1">üéØ Objetivo</h3>
<p>Crear un script que implemente RAG completo usando LangChain + ChromaDB para dar contexto al modelo sobre documentaci√≥n privada.</p>
<h3 id="-prompt-completo-1">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `main.py` que implemente un sistema RAG (Retrieval Augmented Generation) completo usando LangChain y ChromaDB.

REQUISITOS FUNCIONALES:
1. Sistema RAG completo con los siguientes pasos:
   - Cargar documento markdown desde archivo &quot;documentacion_tecnica.md&quot;
   - Dividir documento en chunks (chunk_size=500, chunk_overlap=50)
   - Crear embeddings usando HuggingFaceEmbeddings (modelo: sentence-transformers/all-MiniLM-L6-v2)
   - Almacenar vectores en ChromaDB (directorio: ./chroma_db)
   - Configurar retriever que busque los 3 fragmentos m√°s relevantes
   - Crear cadena RAG usando LangChain Expression Language (LCEL)
2. Detecci√≥n autom√°tica del modelo OpenAI disponible
3. Configuraci√≥n anti-alucinaci√≥n:
   - Temperature: 0.1
   - Prompt template estricto que instruya usar SOLO el contexto proporcionado
4. Chat interactivo que muestre cu√°ntos fragmentos se consultaron
5. Manejo de errores y validaci√≥n de API Key

REQUISITOS T√âCNICOS:
- Librer√≠as: langchain_openai, langchain_community (embeddings, document_loaders, vectorstores), langchain_text_splitters, langchain_core (prompts, runnables, output_parsers), python-dotenv
- Funci√≥n `detectar_modelo()` que pruebe modelos en orden
- Funci√≥n `configurar_rag()` que retorne (rag_chain, modelo, retriever)
- Funci√≥n `main()` con loop interactivo
- Constantes de configuraci√≥n: DOCUMENTO = &quot;documentacion_tecnica.md&quot;, CHROMA_DB_DIR = &quot;./chroma_db&quot;
- Prompt template que diga: &quot;Responde SOLO usando la informaci√≥n que est√° en el contexto proporcionado. Si la informaci√≥n NO est√° en el contexto, di claramente 'No tengo informaci√≥n sobre esto en la documentaci√≥n'&quot;
- Documentaci√≥n en espa√±ol en docstrings
- Mensajes informativos durante la configuraci√≥n

ESTRUCTURA ESPERADA:
- Imports de LangChain
- Constantes de configuraci√≥n
- Funci√≥n detectar_modelo() ‚Üí retorna modelo o None
- Funci√≥n configurar_rag() ‚Üí configura todo el sistema RAG, retorna (rag_chain, modelo, retriever)
- Funci√≥n format_docs(docs) ‚Üí formatea documentos recuperados
- Funci√≥n main() ‚Üí configura RAG y ejecuta chat interactivo
- Punto de entrada: if __name__ == &quot;__main__&quot;

CADENA RAG (LCEL):
Usar LangChain Expression Language:
rag_chain = (
    {&quot;context&quot;: retriever | format_docs, &quot;question&quot;: RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

El script debe demostrar claramente c√≥mo el modelo AHORA S√ç puede responder sobre datos privados usando RAG.
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt-1">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Especifica arquitectura completa</strong>: todos los pasos del pipeline RAG</li>
<li><strong>Detalla configuraciones</strong>: valores espec√≠ficos de chunk_size, modelo de embeddings, etc.</li>
<li><strong>Incluye estructura LCEL</strong>: muestra exactamente c√≥mo construir la cadena</li>
<li><strong>Define prompt template</strong>: texto exacto del prompt para evitar alucinaciones</li>
<li><strong>Especifica retornos</strong>: qu√© debe retornar cada funci√≥n</li>
</ul>
<hr>
<h2 id="3-script-h√≠brido-main_hybridpy">3. Script H√≠brido (<code>main_hybrid.py</code>)</h2>
<h3 id="-objetivo-2">üéØ Objetivo</h3>
<p>Crear un script que combine RAG con conocimiento del modelo: primero busca en documentaci√≥n, si no encuentra usa conocimiento propio.</p>
<h3 id="-prompt-completo-2">üìù Prompt Completo</h3>
<pre><code>Crea un script Python llamado `main_hybrid.py` que implemente un sistema H√çBRIDO que combine RAG con conocimiento del entrenamiento del modelo.

REQUISITOS FUNCIONALES:
1. Sistema h√≠brido con estrategia de decisi√≥n:
   - PRIMERO: Buscar en documentaci√≥n usando RAG
   - EVALUAR: Determinar si los documentos encontrados son relevantes
   - SI ES RELEVANTE: Usar RAG con documentaci√≥n
   - SI NO ES RELEVANTE: Usar conocimiento propio del modelo
   - DETECCI√ìN ESPECIAL: Si el usuario pide expl√≠citamente usar conocimiento fuera de fuentes, hacerlo directamente
2. Configuraci√≥n RAG completa (igual que main.py):
   - Cargar documento, dividir en chunks, crear embeddings HuggingFace, almacenar en ChromaDB
   - Retriever con k=3 fragmentos
3. Funci√≥n de evaluaci√≥n de relevancia:
   - Analizar si documentos recuperados contienen informaci√≥n relevante
   - Verificar contenido sustancial (&gt;200 caracteres) y coincidencias de palabras clave
   - Retornar True/False seg√∫n relevancia
4. Dos modos de respuesta:
   - responder_con_rag(): Usa RAG cuando hay informaci√≥n en documentaci√≥n
   - responder_con_conocimiento_propio(): Usa conocimiento del modelo cuando no hay informaci√≥n relevante
5. Indicadores visuales: Mostrar qu√© fuente se us√≥ (üìö DOCUMENTACI√ìN o üß† CONOCIMIENTO PROPIO)
6. Chat interactivo con informaci√≥n sobre fragmentos consultados

REQUISITOS T√âCNICOS:
- Mismas librer√≠as que main.py (LangChain completo)
- Funci√≥n `configurar_sistema_hibrido()` ‚Üí retorna (retriever, llm, modelo, vectorstore)
- Funci√≥n `evaluar_relevancia_documentos(docs, pregunta)` ‚Üí retorna bool
- Funci√≥n `responder_con_rag(pregunta, retriever, llm)` ‚Üí retorna str
- Funci√≥n `responder_con_conocimiento_propio(pregunta, llm)` ‚Üí retorna str
- Funci√≥n `responder_hibrido(pregunta, retriever, llm)` ‚Üí retorna (respuesta, fuente)
- Funci√≥n `responder_con_rag_directo(pregunta, docs, llm)` ‚Üí fallback cuando RAG falla pero hay docs relevantes
- Detecci√≥n de solicitud expl√≠cita: palabras clave ['fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n']
- Configuraci√≥n anti-alucinaci√≥n: temperature=0.1
- Prompts espec√≠ficos:
  - RAG: &quot;DEBES responder usando la informaci√≥n del contexto proporcionado. NO uses conocimiento fuera del contexto&quot;
  - Conocimiento propio: &quot;Responde usando tu conocimiento general. Si NO sabes, di 'No s√© sobre...'&quot;

ESTRATEGIA DE EVALUACI√ìN DE RELEVANCIA:
- Si contenido_total &gt; 200 caracteres Y hay palabras clave importantes en el contenido ‚Üí relevante
- Si contenido_total &gt;= 50 Y hay coincidencias de palabras clave ‚Üí relevante
- Filtrar stopwords en espa√±ol antes de evaluar palabras clave
- Confiar en el retriever cuando encuentra contenido sustancial

ESTRUCTURA ESPERADA:
- Imports completos de LangChain
- Constantes: DOCUMENTO, CHROMA_DB_DIR
- detectar_modelo()
- configurar_sistema_hibrido()
- evaluar_relevancia_documentos(docs, pregunta)
- responder_con_rag(pregunta, retriever, llm)
- responder_con_rag_directo(pregunta, docs, llm) [fallback]
- responder_con_conocimiento_propio(pregunta, llm)
- responder_hibrido(pregunta, retriever, llm) [funci√≥n principal de decisi√≥n]
- main()
- Punto de entrada

L√ìGICA DE responder_hibrido():
1. Detectar si usuario pide expl√≠citamente conocimiento fuera ‚Üí usar conocimiento propio directamente
2. Buscar documentos con retriever
3. Evaluar relevancia con evaluar_relevancia_documentos()
4. Si relevante ‚Üí usar RAG
5. Si RAG dice &quot;no tengo informaci√≥n&quot; pero hay docs relevantes ‚Üí intentar responder_con_rag_directo()
6. Si no relevante ‚Üí usar conocimiento propio
7. Retornar (respuesta, fuente) donde fuente es &quot;documentaci√≥n&quot; o &quot;conocimiento del modelo&quot;

El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo.
</code></pre>
<h3 id="-caracter√≠sticas-clave-del-prompt-2">‚úÖ Caracter√≠sticas Clave del Prompt</h3>
<ul>
<li><strong>Define estrategia completa</strong>: flujo de decisi√≥n paso a paso</li>
<li><strong>Especifica m√∫ltiples funciones</strong>: cada una con prop√≥sito claro</li>
<li><strong>Detalla l√≥gica de evaluaci√≥n</strong>: c√≥mo determinar relevancia</li>
<li><strong>Incluye casos especiales</strong>: detecci√≥n de solicitud expl√≠cita, fallback</li>
<li><strong>Define estructura completa</strong>: todas las funciones necesarias</li>
</ul>
<hr>
<h2 id="-mejores-pr√°cticas-de-prompt-engineering">üéì Mejores Pr√°cticas de Prompt Engineering</h2>
<h3 id="1-estructura-clara">1. <strong>Estructura Clara</strong></h3>
<ul>
<li>Separar en secciones: REQUISITOS FUNCIONALES, REQUISITOS T√âCNICOS, ESTRUCTURA ESPERADA</li>
<li>Usar listas numeradas o con vi√±etas para mejor legibilidad</li>
</ul>
<h3 id="2-especificidad">2. <strong>Especificidad</strong></h3>
<ul>
<li>Incluir valores exactos: <code>temperature=0.1</code>, <code>chunk_size=500</code></li>
<li>Mencionar librer√≠as espec√≠ficas: <code>langchain_openai</code>, <code>HuggingFaceEmbeddings</code></li>
<li>Definir nombres de funciones y variables esperadas</li>
</ul>
<h3 id="3-contexto-y-objetivo">3. <strong>Contexto y Objetivo</strong></h3>
<ul>
<li>Empezar con el objetivo educativo o funcional</li>
<li>Explicar el &quot;por qu√©&quot; adem√°s del &quot;qu√©&quot;</li>
</ul>
<h3 id="4-ejemplos-concretos">4. <strong>Ejemplos Concretos</strong></h3>
<ul>
<li>Incluir c√≥digo de ejemplo cuando sea relevante (como la cadena LCEL)</li>
<li>Mostrar estructuras esperadas</li>
</ul>
<h3 id="5-restricciones-y-validaciones">5. <strong>Restricciones y Validaciones</strong></h3>
<ul>
<li>Especificar qu√© NO debe hacer (evitar alucinaciones)</li>
<li>Definir manejo de errores esperado</li>
</ul>
<h3 id="6-documentaci√≥n-esperada">6. <strong>Documentaci√≥n Esperada</strong></h3>
<ul>
<li>Solicitar docstrings en espa√±ol</li>
<li>Pedir mensajes informativos para UX</li>
</ul>
<hr>
<h2 id="-comparaci√≥n-de-prompts">üìä Comparaci√≥n de Prompts</h2>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>Script Sin Contexto</th>
<th>Script con RAG</th>
<th>Script H√≠brido</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Complejidad</strong></td>
<td>Baja</td>
<td>Media</td>
<td>Alta</td>
</tr>
<tr>
<td><strong>Librer√≠as</strong></td>
<td>2 (openai, dotenv)</td>
<td>7+ (LangChain completo)</td>
<td>7+ (LangChain completo)</td>
</tr>
<tr>
<td><strong>Funciones</strong></td>
<td>2</td>
<td>3</td>
<td>8+</td>
</tr>
<tr>
<td><strong>L√≥gica de Decisi√≥n</strong></td>
<td>Ninguna</td>
<td>Simple (solo RAG)</td>
<td>Compleja (RAG + evaluaci√≥n + fallback)</td>
</tr>
<tr>
<td><strong>Prompt Length</strong></td>
<td>~300 palabras</td>
<td>~500 palabras</td>
<td>~800 palabras</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="-uso-de-los-prompts">üöÄ Uso de los Prompts</h2>
<h3 id="opci√≥n-1-copiar-y-pegar-directo">Opci√≥n 1: Copiar y Pegar Directo</h3>
<p>Copia el prompt completo en tu herramienta de IA favorita (Claude, GPT-4, etc.) y solicita la generaci√≥n del c√≥digo.</p>
<h3 id="opci√≥n-2-adaptaci√≥n-incremental">Opci√≥n 2: Adaptaci√≥n Incremental</h3>
<ol>
<li>Empieza con el prompt base</li>
<li>Agrega requisitos espec√≠ficos de tu proyecto</li>
<li>Refina seg√∫n necesidades</li>
</ol>
<h3 id="opci√≥n-3-prompt-modular">Opci√≥n 3: Prompt Modular</h3>
<p>Divide el prompt en secciones y genera cada parte por separado, luego combina.</p>
<hr>
<h2 id="-tips-adicionales">üí° Tips Adicionales</h2>
<ol>
<li><strong>Iteraci√≥n</strong>: Los prompts pueden necesitar refinamiento. Prueba y ajusta.</li>
<li><strong>Especificidad</strong>: Mientras m√°s espec√≠fico, mejor resultado. Incluye valores exactos.</li>
<li><strong>Ejemplos</strong>: Si tienes c√≥digo de referencia, incl√∫yelo en el prompt.</li>
<li><strong>Validaci√≥n</strong>: Siempre prueba el c√≥digo generado antes de usarlo en producci√≥n.</li>
<li><strong>Documentaci√≥n</strong>: Solicita expl√≠citamente documentaci√≥n en espa√±ol si la necesitas.</li>
</ol>
<hr>
<h2 id="-notas-finales">üìù Notas Finales</h2>
<p>Estos prompts est√°n dise√±ados para generar c√≥digo funcional y bien documentado. Sin embargo, siempre:</p>
<ul>
<li>‚úÖ Revisa el c√≥digo generado</li>
<li>‚úÖ Prueba la funcionalidad</li>
<li>‚úÖ Ajusta seg√∫n tus necesidades espec√≠ficas</li>
<li>‚úÖ Valida dependencias y configuraciones</li>
</ul>
<p>Los prompts pueden adaptarse para otros frameworks o lenguajes cambiando las librer√≠as y estructuras mencionadas.</p>

            
            
        </body>
        </html>