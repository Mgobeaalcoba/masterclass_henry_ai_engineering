# üéØ Prompt Engineering: Generaci√≥n de Scripts RAG

Este documento contiene ejemplos de **prompt engineering** para generar los tres scripts principales del proyecto usando un solo prompt cada uno.

---

## üìã √çndice

1. [Script sin Contexto](#1-script-sin-contexto-model_without_contextpy)
2. [Script con RAG](#2-script-con-rag-mainpy)
3. [Script H√≠brido](#3-script-h√≠brido-main_hybridpy)

---

## 1. Script sin Contexto (`model_without_context.py`)

### üéØ Objetivo
Crear un script simple que demuestre c√≥mo un LLM **NO puede responder** sobre informaci√≥n privada porque no tiene acceso a documentaci√≥n interna.

### üìù Prompt Completo

```
Crea un script Python llamado `model_without_context.py` que demuestre la falta de contexto en LLMs.

REQUISITOS FUNCIONALES:
1. Chat interactivo con OpenAI GPT usando la API oficial
2. Detecci√≥n autom√°tica del modelo disponible (gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)
3. Configuraci√≥n anti-alucinaci√≥n:
   - Temperature: 0.1 (muy baja)
   - top_p: 0.9
   - System prompt que instruya al modelo a decir "No s√© sobre..." cuando no tiene informaci√≥n
4. Loop de conversaci√≥n interactivo con comando 'salir' para terminar
5. Manejo de errores y validaci√≥n de API Key desde archivo .env

REQUISITOS T√âCNICOS:
- Usar `openai` (cliente oficial) y `python-dotenv`
- Funci√≥n `detectar_modelo(client)` que pruebe modelos en orden de preferencia
- Funci√≥n `main()` con loop interactivo
- System prompt que evite alucinaciones: "Eres un asistente honesto y directo. Si no conoces o no tienes informaci√≥n sobre algo, debes decir claramente 'No s√© sobre...' o 'No tengo informaci√≥n sobre...' en lugar de inventar o suponer."
- Documentaci√≥n en espa√±ol en docstrings
- Mensajes informativos con emojis para mejor UX

ESTRUCTURA ESPERADA:
- Imports: os, OpenAI, load_dotenv
- Funci√≥n detectar_modelo(client) ‚Üí retorna modelo disponible o None
- Funci√≥n main() ‚Üí configuraci√≥n, loop interactivo, manejo de errores
- Punto de entrada: if __name__ == "__main__"

El script debe ser simple, directo y demostrar claramente que el modelo NO conoce datos privados.
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Especifica el objetivo educativo**: "demostrar la falta de contexto"
- **Lista requisitos funcionales**: qu√© debe hacer el script
- **Lista requisitos t√©cnicos**: librer√≠as y configuraciones espec√≠ficas
- **Define estructura**: funciones esperadas y organizaci√≥n
- **Incluye detalles espec√≠ficos**: valores de temperatura, system prompt exacto

---

## 2. Script con RAG (`main.py`)

### üéØ Objetivo
Crear un script que implemente RAG completo usando LangChain + ChromaDB para dar contexto al modelo sobre documentaci√≥n privada.

### üìù Prompt Completo

```
Crea un script Python llamado `main.py` que implemente un sistema RAG (Retrieval Augmented Generation) completo usando LangChain y ChromaDB.

REQUISITOS FUNCIONALES:
1. Sistema RAG completo con los siguientes pasos:
   - Cargar documento markdown desde archivo "documentacion_tecnica.md"
   - Dividir documento en chunks (chunk_size=500, chunk_overlap=50)
   - Crear embeddings usando HuggingFaceEmbeddings (modelo: sentence-transformers/all-MiniLM-L6-v2)
   - Almacenar vectores en ChromaDB (directorio: ./chroma_db)
   - Configurar retriever que busque los 3 fragmentos m√°s relevantes
   - Crear cadena RAG usando LangChain Expression Language (LCEL)
2. Detecci√≥n autom√°tica del modelo OpenAI disponible
3. Configuraci√≥n anti-alucinaci√≥n:
   - Temperature: 0.1
   - Prompt template estricto que instruya usar SOLO el contexto proporcionado
4. Chat interactivo que muestre cu√°ntos fragmentos se consultaron
5. Manejo de errores y validaci√≥n de API Key

REQUISITOS T√âCNICOS:
- Librer√≠as: langchain_openai, langchain_community (embeddings, document_loaders, vectorstores), langchain_text_splitters, langchain_core (prompts, runnables, output_parsers), python-dotenv
- Funci√≥n `detectar_modelo()` que pruebe modelos en orden
- Funci√≥n `configurar_rag()` que retorne (rag_chain, modelo, retriever)
- Funci√≥n `main()` con loop interactivo
- Constantes de configuraci√≥n: DOCUMENTO = "documentacion_tecnica.md", CHROMA_DB_DIR = "./chroma_db"
- Prompt template que diga: "Responde SOLO usando la informaci√≥n que est√° en el contexto proporcionado. Si la informaci√≥n NO est√° en el contexto, di claramente 'No tengo informaci√≥n sobre esto en la documentaci√≥n'"
- Documentaci√≥n en espa√±ol en docstrings
- Mensajes informativos durante la configuraci√≥n

ESTRUCTURA ESPERADA:
- Imports de LangChain
- Constantes de configuraci√≥n
- Funci√≥n detectar_modelo() ‚Üí retorna modelo o None
- Funci√≥n configurar_rag() ‚Üí configura todo el sistema RAG, retorna (rag_chain, modelo, retriever)
- Funci√≥n format_docs(docs) ‚Üí formatea documentos recuperados
- Funci√≥n main() ‚Üí configura RAG y ejecuta chat interactivo
- Punto de entrada: if __name__ == "__main__"

CADENA RAG (LCEL):
Usar LangChain Expression Language:
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

El script debe demostrar claramente c√≥mo el modelo AHORA S√ç puede responder sobre datos privados usando RAG.
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Especifica arquitectura completa**: todos los pasos del pipeline RAG
- **Detalla configuraciones**: valores espec√≠ficos de chunk_size, modelo de embeddings, etc.
- **Incluye estructura LCEL**: muestra exactamente c√≥mo construir la cadena
- **Define prompt template**: texto exacto del prompt para evitar alucinaciones
- **Especifica retornos**: qu√© debe retornar cada funci√≥n

---

## 3. Script H√≠brido (`main_hybrid.py`)

### üéØ Objetivo
Crear un script que combine RAG con conocimiento del modelo: primero busca en documentaci√≥n, si no encuentra usa conocimiento propio.

### üìù Prompt Completo

```
Crea un script Python llamado `main_hybrid.py` que implemente un sistema H√çBRIDO que combine RAG con conocimiento del entrenamiento del modelo.

REQUISITOS FUNCIONALES:
1. Sistema h√≠brido con estrategia de decisi√≥n:
   - PRIMERO: Buscar en documentaci√≥n usando RAG
   - EVALUAR: Determinar si los documentos encontrados son relevantes
   - SI ES RELEVANTE: Usar RAG con documentaci√≥n
   - SI NO ES RELEVANTE: Usar conocimiento propio del modelo
   - DETECCI√ìN ESPECIAL: Si el usuario pide expl√≠citamente usar conocimiento fuera de fuentes, hacerlo directamente
2. Configuraci√≥n RAG completa (igual que main.py):
   - Cargar documento, dividir en chunks, crear embeddings HuggingFace, almacenar en ChromaDB
   - Retriever con k=3 fragmentos
3. Funci√≥n de evaluaci√≥n de relevancia:
   - Analizar si documentos recuperados contienen informaci√≥n relevante
   - Verificar contenido sustancial (>200 caracteres) y coincidencias de palabras clave
   - Retornar True/False seg√∫n relevancia
4. Dos modos de respuesta:
   - responder_con_rag(): Usa RAG cuando hay informaci√≥n en documentaci√≥n
   - responder_con_conocimiento_propio(): Usa conocimiento del modelo cuando no hay informaci√≥n relevante
5. Indicadores visuales: Mostrar qu√© fuente se us√≥ (üìö DOCUMENTACI√ìN o üß† CONOCIMIENTO PROPIO)
6. Chat interactivo con informaci√≥n sobre fragmentos consultados

REQUISITOS T√âCNICOS:
- Mismas librer√≠as que main.py (LangChain completo)
- Funci√≥n `configurar_sistema_hibrido()` ‚Üí retorna (retriever, llm, modelo, vectorstore)
- Funci√≥n `evaluar_relevancia_documentos(docs, pregunta)` ‚Üí retorna bool
- Funci√≥n `responder_con_rag(pregunta, retriever, llm)` ‚Üí retorna str
- Funci√≥n `responder_con_conocimiento_propio(pregunta, llm)` ‚Üí retorna str
- Funci√≥n `responder_hibrido(pregunta, retriever, llm)` ‚Üí retorna (respuesta, fuente)
- Funci√≥n `responder_con_rag_directo(pregunta, docs, llm)` ‚Üí fallback cuando RAG falla pero hay docs relevantes
- Detecci√≥n de solicitud expl√≠cita: palabras clave ['fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n']
- Configuraci√≥n anti-alucinaci√≥n: temperature=0.1
- Prompts espec√≠ficos:
  - RAG: "DEBES responder usando la informaci√≥n del contexto proporcionado. NO uses conocimiento fuera del contexto"
  - Conocimiento propio: "Responde usando tu conocimiento general. Si NO sabes, di 'No s√© sobre...'"

ESTRATEGIA DE EVALUACI√ìN DE RELEVANCIA:
- Si contenido_total > 200 caracteres Y hay palabras clave importantes en el contenido ‚Üí relevante
- Si contenido_total >= 50 Y hay coincidencias de palabras clave ‚Üí relevante
- Filtrar stopwords en espa√±ol antes de evaluar palabras clave
- Confiar en el retriever cuando encuentra contenido sustancial

ESTRUCTURA ESPERADA:
- Imports completos de LangChain
- Constantes: DOCUMENTO, CHROMA_DB_DIR
- detectar_modelo()
- configurar_sistema_hibrido()
- evaluar_relevancia_documentos(docs, pregunta)
- responder_con_rag(pregunta, retriever, llm)
- responder_con_rag_directo(pregunta, docs, llm) [fallback]
- responder_con_conocimiento_propio(pregunta, llm)
- responder_hibrido(pregunta, retriever, llm) [funci√≥n principal de decisi√≥n]
- main()
- Punto de entrada

L√ìGICA DE responder_hibrido():
1. Detectar si usuario pide expl√≠citamente conocimiento fuera ‚Üí usar conocimiento propio directamente
2. Buscar documentos con retriever
3. Evaluar relevancia con evaluar_relevancia_documentos()
4. Si relevante ‚Üí usar RAG
5. Si RAG dice "no tengo informaci√≥n" pero hay docs relevantes ‚Üí intentar responder_con_rag_directo()
6. Si no relevante ‚Üí usar conocimiento propio
7. Retornar (respuesta, fuente) donde fuente es "documentaci√≥n" o "conocimiento del modelo"

El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo.
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Define estrategia completa**: flujo de decisi√≥n paso a paso
- **Especifica m√∫ltiples funciones**: cada una con prop√≥sito claro
- **Detalla l√≥gica de evaluaci√≥n**: c√≥mo determinar relevancia
- **Incluye casos especiales**: detecci√≥n de solicitud expl√≠cita, fallback
- **Define estructura completa**: todas las funciones necesarias

---

## üéì Mejores Pr√°cticas de Prompt Engineering

### 1. **Estructura Clara**
- Separar en secciones: REQUISITOS FUNCIONALES, REQUISITOS T√âCNICOS, ESTRUCTURA ESPERADA
- Usar listas numeradas o con vi√±etas para mejor legibilidad

### 2. **Especificidad**
- Incluir valores exactos: `temperature=0.1`, `chunk_size=500`
- Mencionar librer√≠as espec√≠ficas: `langchain_openai`, `HuggingFaceEmbeddings`
- Definir nombres de funciones y variables esperadas

### 3. **Contexto y Objetivo**
- Empezar con el objetivo educativo o funcional
- Explicar el "por qu√©" adem√°s del "qu√©"

### 4. **Ejemplos Concretos**
- Incluir c√≥digo de ejemplo cuando sea relevante (como la cadena LCEL)
- Mostrar estructuras esperadas

### 5. **Restricciones y Validaciones**
- Especificar qu√© NO debe hacer (evitar alucinaciones)
- Definir manejo de errores esperado

### 6. **Documentaci√≥n Esperada**
- Solicitar docstrings en espa√±ol
- Pedir mensajes informativos para UX

---

## üìä Comparaci√≥n de Prompts

| Aspecto | Script Sin Contexto | Script con RAG | Script H√≠brido |
|---------|-------------------|----------------|----------------|
| **Complejidad** | Baja | Media | Alta |
| **Librer√≠as** | 2 (openai, dotenv) | 7+ (LangChain completo) | 7+ (LangChain completo) |
| **Funciones** | 2 | 3 | 8+ |
| **L√≥gica de Decisi√≥n** | Ninguna | Simple (solo RAG) | Compleja (RAG + evaluaci√≥n + fallback) |
| **Prompt Length** | ~300 palabras | ~500 palabras | ~800 palabras |

---

## üöÄ Uso de los Prompts

### Opci√≥n 1: Copiar y Pegar Directo
Copia el prompt completo en tu herramienta de IA favorita (Claude, GPT-4, etc.) y solicita la generaci√≥n del c√≥digo.

### Opci√≥n 2: Adaptaci√≥n Incremental
1. Empieza con el prompt base
2. Agrega requisitos espec√≠ficos de tu proyecto
3. Refina seg√∫n necesidades

### Opci√≥n 3: Prompt Modular
Divide el prompt en secciones y genera cada parte por separado, luego combina.

---

## üí° Tips Adicionales

1. **Iteraci√≥n**: Los prompts pueden necesitar refinamiento. Prueba y ajusta.
2. **Especificidad**: Mientras m√°s espec√≠fico, mejor resultado. Incluye valores exactos.
3. **Ejemplos**: Si tienes c√≥digo de referencia, incl√∫yelo en el prompt.
4. **Validaci√≥n**: Siempre prueba el c√≥digo generado antes de usarlo en producci√≥n.
5. **Documentaci√≥n**: Solicita expl√≠citamente documentaci√≥n en espa√±ol si la necesitas.

---

## üìù Notas Finales

Estos prompts est√°n dise√±ados para generar c√≥digo funcional y bien documentado. Sin embargo, siempre:

- ‚úÖ Revisa el c√≥digo generado
- ‚úÖ Prueba la funcionalidad
- ‚úÖ Ajusta seg√∫n tus necesidades espec√≠ficas
- ‚úÖ Valida dependencias y configuraciones

Los prompts pueden adaptarse para otros frameworks o lenguajes cambiando las librer√≠as y estructuras mencionadas.

