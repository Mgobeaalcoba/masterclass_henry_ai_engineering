# üéØ Prompt Engineering: Generaci√≥n de Scripts RAG

Este documento contiene ejemplos de **prompt engineering** para generar los tres scripts principales del proyecto usando un solo prompt cada uno.

---

## üìã √çndice

1. [Script sin Contexto](#1-script-sin-contexto-model_without_contextpy)
2. [Script con RAG](#2-script-con-rag-mainpy)
3. [Script H√≠brido](#3-script-h√≠brido-main_hybridpy)

---

## 1. Script sin Contexto (`model_without_context.py`)

### üéØ Objetivo
Crear un script simple que demuestre c√≥mo un LLM **NO puede responder** sobre informaci√≥n privada porque no tiene acceso a documentaci√≥n interna.

### üìù Prompt Completo

```
Crea un script Python llamado `model_without_context.py` que demuestre la falta de contexto en LLMs.

OBJETIVO DEL SISTEMA:
El script debe demostrar que los modelos de lenguaje grandes (LLMs) NO pueden responder sobre informaci√≥n privada de empresas porque no tienen acceso a documentaci√≥n interna. El sistema debe implementar un chat interactivo simple con OpenAI GPT que muestre claramente esta limitaci√≥n educativa.

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- openai 2.8.0 o superior
- python-dotenv 1.2.1 o superior

REQUISITOS FUNCIONALES:

1. DETECCI√ìN AUTOM√ÅTICA DE MODELO:
   - El sistema debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible en la cuenta del usuario
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible y continuar con el siguiente
   - Debe informar al usuario qu√© modelo se est√° utilizando

2. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar mensajes informativos durante el proceso de consulta

3. CONFIGURACI√ìN ANTI-ALUCINACI√ìN:
   - El sistema debe configurar el modelo con temperatura muy baja (0.1) para minimizar creatividad y alucinaciones
   - Debe usar top_p de 0.9 para nucleus sampling m√°s restrictivo
   - Debe limitar las respuestas a m√°ximo 500 tokens
   - Debe incluir un prompt del sistema que instruya expl√≠citamente al modelo a decir "No s√© sobre..." cuando no tiene informaci√≥n, en lugar de inventar o suponer

4. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el chat
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

5. MANEJO DE ERRORES:
   - El sistema debe manejar errores de conexi√≥n, autenticaci√≥n y otros errores de API
   - Debe mostrar mensajes de error amigables al usuario sin exponer detalles t√©cnicos internos
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. ESTRUCTURA DEL ARCHIVO:
   - Debe incluir shebang para Python
   - Debe tener docstring principal que explique el prop√≥sito educativo del script
   - Debe organizarse en funciones claramente definidas

2. IMPORTS REQUERIDOS:
   - M√≥dulo os del sistema est√°ndar de Python
   - Clase OpenAI del paquete openai
   - Funci√≥n load_dotenv del paquete dotenv

3. CONFIGURACI√ìN DEL MODELO:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - top_p: 0.9 (nucleus sampling restrictivo)
   - System prompt: Debe instruir al modelo a ser honesto y decir "No s√© sobre..." cuando no tiene informaci√≥n

4. MENSAJES DE USUARIO:
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Debe mostrar claramente el nombre del modelo en uso
   - Debe incluir separadores visuales entre conversaciones
   - Mensajes deben estar en espa√±ol

5. DOCUMENTACI√ìN:
   - Todas las funciones deben tener docstrings en espa√±ol
   - Comentarios en espa√±ol cuando sean necesarios
   - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar claramente que el modelo NO conoce datos privados de empresas, permitiendo al usuario hacer preguntas interactivamente y observando c√≥mo el modelo responde honestamente cuando no tiene informaci√≥n, sin alucinar o inventar detalles.
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Especifica el objetivo educativo**: "demostrar la falta de contexto"
- **Lista requisitos funcionales**: qu√© debe hacer el script
- **Lista requisitos t√©cnicos**: librer√≠as y configuraciones espec√≠ficas
- **Define estructura**: funciones esperadas y organizaci√≥n
- **Incluye detalles espec√≠ficos**: valores de temperatura, system prompt exacto

---

## 2. Script con RAG (`main.py`)

### üéØ Objetivo
Crear un script que implemente RAG completo usando LangChain + ChromaDB para dar contexto al modelo sobre documentaci√≥n privada.

### üìù Prompt Completo

```
Crea un script Python llamado `main.py` que implemente un sistema RAG (Retrieval Augmented Generation) completo usando LangChain y ChromaDB.

OBJETIVO DEL SISTEMA:
El script debe demostrar c√≥mo darle contexto a GPT sobre documentaci√≥n privada usando LangChain + ChromaDB. El sistema debe permitir que el modelo responda sobre datos internos que no est√°n en su entrenamiento mediante la implementaci√≥n de un sistema RAG completo que carga documentaci√≥n t√©cnica, la procesa, y permite hacer preguntas interactivas que se responden usando el contexto de la documentaci√≥n.

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- langchain 1.0.7 o superior
- langchain-openai 1.0.3 o superior
- langchain-community 0.4.1 o superior
- langchain-huggingface 1.0.0 o superior (CR√çTICO: usar esta versi√≥n para evitar deprecaciones de HuggingFaceEmbeddings)
- chromadb 1.3.4 o superior
- python-dotenv 1.2.1 o superior
- openai 2.8.0 o superior
- sentence-transformers 5.1.2 o superior

REQUISITOS FUNCIONALES:

1. CARGA Y PROCESAMIENTO DE DOCUMENTACI√ìN:
   - El sistema debe cargar un documento markdown desde el archivo "documentacion_tecnica.md"
   - Debe dividir el documento en fragmentos (chunks) de tama√±o 500 caracteres con solapamiento de 50 caracteres
   - Debe crear embeddings vectoriales usando el modelo HuggingFace "sentence-transformers/all-MiniLM-L6-v2"
   - Debe almacenar los vectores en una base de datos vectorial ChromaDB persistente en el directorio "./chroma_db"
   - Debe mostrar mensajes informativos durante cada paso del proceso de configuraci√≥n

2. SISTEMA DE RECUPERACI√ìN (RETRIEVER):
   - El sistema debe configurar un retriever que busque los 3 fragmentos m√°s relevantes para cada pregunta
   - Debe usar b√∫squeda por similitud sem√°ntica basada en embeddings
   - Debe informar al usuario cu√°ntos fragmentos se consultaron para cada respuesta

3. CADENA RAG CON LANGCHAIN EXPRESSION LANGUAGE (LCEL):
   - El sistema debe implementar una cadena RAG usando LCEL (LangChain Expression Language)
   - La cadena debe combinar: recuperaci√≥n de contexto ‚Üí formateo de documentos ‚Üí prompt template ‚Üí modelo LLM ‚Üí parser de salida
   - Debe usar el operador pipe de Python para componer la cadena
   - CR√çTICO: Debe usar la API moderna de LangChain (v1.0+), NO m√©todos deprecados

4. DETECCI√ìN AUTOM√ÅTICA DE MODELO:
   - El sistema debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible

5. CONFIGURACI√ìN ANTI-ALUCINACI√ìN:
   - El sistema debe configurar el modelo con temperatura 0.1 para minimizar alucinaciones
   - Debe limitar las respuestas a m√°ximo 500 tokens
   - Debe incluir un prompt template estricto que instruya al modelo a usar SOLO el contexto proporcionado
   - El prompt debe indicar expl√≠citamente que si la informaci√≥n NO est√° en el contexto, debe decir "No tengo informaci√≥n sobre esto en la documentaci√≥n"

6. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar qu√© fragmentos de documentaci√≥n se consultaron para cada respuesta

7. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el sistema RAG
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

8. MANEJO DE ERRORES:
   - El sistema debe manejar errores de carga de documento, creaci√≥n de embeddings, y consultas al modelo
   - Debe mostrar mensajes de error amigables con sugerencias de soluci√≥n
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. CONFIGURACI√ìN DE TOKENIZERS:
   - El sistema debe configurar la variable de entorno TOKENIZERS_PARALLELISM="false" antes de importar cualquier m√≥dulo de HuggingFace
   - Esto evita warnings de paralelismo despu√©s de fork

2. IMPORTS REQUERIDOS:
   - M√≥dulo os del sistema est√°ndar de Python
   - Funci√≥n load_dotenv del paquete dotenv
   - Clase ChatOpenAI del paquete langchain_openai
   - Clase HuggingFaceEmbeddings del paquete langchain_huggingface (CR√çTICO: NO usar langchain_community.embeddings que est√° deprecado)
   - Clase TextLoader del paquete langchain_community.document_loaders
   - Clase Chroma del paquete langchain_community.vectorstores
   - Clase RecursiveCharacterTextSplitter del paquete langchain_text_splitters
   - Clase ChatPromptTemplate del paquete langchain_core.prompts
   - Clase RunnablePassthrough del paquete langchain_core.runnables
   - Clase StrOutputParser del paquete langchain_core.output_parsers

3. CONFIGURACI√ìN DE CHUNKS:
   - Tama√±o de chunk: 500 caracteres
   - Solapamiento entre chunks: 50 caracteres
   - Funci√≥n de longitud: len (contar caracteres)

4. CONFIGURACI√ìN DE EMBEDDINGS:
   - Modelo: sentence-transformers/all-MiniLM-L6-v2
   - Dispositivo: CPU
   - Proveedor: HuggingFace (gratuito, sin costo)

5. CONFIGURACI√ìN DEL MODELO LLM:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - Modelo: Detectado autom√°ticamente de la lista disponible

6. PROMPT TEMPLATE:
   - Debe instruir al modelo a responder √öNICAMENTE usando la documentaci√≥n proporcionada
   - Debe incluir instrucciones expl√≠citas para evitar alucinaciones
   - Debe indicar que si no hay informaci√≥n en el contexto, debe decir claramente "No tengo informaci√≥n sobre esto en la documentaci√≥n"

7. ESTRUCTURA DE ARCHIVOS:
   - Archivo de documentaci√≥n: "documentacion_tecnica.md"
   - Directorio de base vectorial: "./chroma_db"
   - Archivo de configuraci√≥n: ".env" (para API Key)

8. API MODERNA DE LANGCHAIN:
   - CR√çTICO: Usar langchain_huggingface para HuggingFaceEmbeddings (NO langchain_community.embeddings)
   - CR√çTICO: Usar retriever.invoke() para buscar documentos (NO m√©todos deprecados como get_relevant_documents)
   - CR√çTICO: Usar LangChain Expression Language (LCEL) con operador pipe para construir cadenas

9. MENSAJES DE USUARIO:
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Debe mostrar claramente el nombre del modelo en uso
   - Debe incluir separadores visuales entre conversaciones
   - Mensajes deben estar en espa√±ol

10. DOCUMENTACI√ìN:
    - Todas las funciones deben tener docstrings en espa√±ol
    - Comentarios en espa√±ol cuando sean necesarios
    - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar claramente c√≥mo el modelo AHORA S√ç puede responder sobre datos privados usando RAG, permitiendo al usuario hacer preguntas interactivamente y observando c√≥mo el modelo responde usando el contexto de la documentaci√≥n t√©cnica, sin alucinar informaci√≥n que no est√° en la documentaci√≥n.

FUNCIONALIDADES ESPERADAS:

1. FUNCI√ìN DE DETECCI√ìN DE MODELO:
   - El sistema debe implementar una funci√≥n que detecte autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe probar modelos en orden de preferencia: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
   - Debe manejar errores cuando un modelo no est√° disponible y continuar con el siguiente
   - Debe retornar el nombre del modelo disponible o None si ninguno est√° disponible

2. FUNCI√ìN DE CONFIGURACI√ìN RAG:
   - El sistema debe implementar una funci√≥n que configure el sistema RAG completo
   - La funci√≥n debe realizar los siguientes pasos en orden:
     a) Cargar documento markdown desde archivo
     b) Dividir documento en fragmentos con tama√±o y solapamiento especificados
     c) Crear embeddings vectoriales usando HuggingFace
     d) Almacenar vectores en ChromaDB persistente
     e) Detectar y configurar modelo LLM disponible
     f) Crear retriever que busque fragmentos relevantes
     g) Crear prompt template con instrucciones estrictas
     h) Construir cadena RAG usando LangChain Expression Language
   - La funci√≥n debe mostrar mensajes informativos durante cada paso
   - La funci√≥n debe retornar una tupla con (rag_chain, modelo, retriever)

3. FUNCI√ìN PRINCIPAL:
   - El sistema debe implementar una funci√≥n principal que orqueste todo el flujo
   - Debe validar la existencia de la API Key antes de iniciar
   - Debe configurar el sistema RAG llamando a la funci√≥n de configuraci√≥n
   - Debe mostrar mensajes de bienvenida e instrucciones al usuario
   - Debe implementar un loop interactivo que permita hacer preguntas continuamente
   - Debe procesar cada pregunta usando la cadena RAG
   - Debe mostrar la respuesta junto con informaci√≥n sobre los fragmentos consultados
   - Debe manejar errores de forma amigable y permitir continuar despu√©s de errores
   - Debe reconocer comandos de salida y terminar el programa apropiadamente

4. MENSAJES Y FEEDBACK AL USUARIO:
   - El sistema debe mostrar mensajes informativos durante la configuraci√≥n del RAG
   - Debe mostrar mensajes de bienvenida con informaci√≥n sobre el sistema
   - Debe mostrar mensajes durante el procesamiento de cada pregunta
   - Debe mostrar claramente qu√© fragmentos de documentaci√≥n se consultaron
   - Todos los mensajes deben incluir emojis apropiados para mejor UX
   - Mensajes deben estar en espa√±ol

5. ESTRUCTURA Y ORGANIZACI√ìN:
   - El c√≥digo debe estar organizado en funciones claramente definidas
   - Cada funci√≥n debe tener docstrings en espa√±ol explicando su prop√≥sito
   - El c√≥digo debe seguir convenciones PEP 8
   - Debe incluir punto de entrada est√°ndar de Python
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Especifica arquitectura completa**: todos los pasos del pipeline RAG
- **Detalla configuraciones**: valores espec√≠ficos de chunk_size, modelo de embeddings, etc.
- **Incluye estructura LCEL**: muestra exactamente c√≥mo construir la cadena
- **Define prompt template**: texto exacto del prompt para evitar alucinaciones
- **Especifica retornos**: qu√© debe retornar cada funci√≥n

---

## 3. Script H√≠brido (`main_hybrid.py`)

### üéØ Objetivo
Crear un script que combine RAG con conocimiento del modelo: primero busca en documentaci√≥n, si no encuentra usa conocimiento propio.

### üìù Prompt Completo

```
Crea un script Python llamado `main_hybrid.py` que implemente un sistema H√çBRIDO que combine RAG con conocimiento del entrenamiento del modelo.

OBJETIVO DEL SISTEMA:
El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo. El sistema debe implementar una estrategia h√≠brida que primero busca informaci√≥n en documentaci√≥n privada usando RAG, y si no encuentra informaci√≥n relevante, usa el conocimiento del entrenamiento del modelo. Si el modelo tampoco sabe, debe decir expl√≠citamente "No s√©". El sistema debe mostrar claramente qu√© fuente utiliz√≥ para responder cada pregunta (documentaci√≥n o conocimiento propio).

DEPENDENCIAS REQUERIDAS:
El proyecto utiliza Poetry como gestor de dependencias. Las dependencias necesarias son:
- Python 3.13 o superior
- langchain 1.0.7 o superior
- langchain-openai 1.0.3 o superior
- langchain-community 0.4.1 o superior
- langchain-huggingface 1.0.0 o superior (CR√çTICO: usar esta versi√≥n para evitar deprecaciones de HuggingFaceEmbeddings)
- chromadb 1.3.4 o superior
- python-dotenv 1.2.1 o superior
- openai 2.8.0 o superior
- sentence-transformers 5.1.2 o superior

REQUISITOS FUNCIONALES:

1. CONFIGURACI√ìN DEL SISTEMA H√çBRIDO:
   - El sistema debe configurar un sistema RAG completo id√©ntico al script main.py (carga de documento, chunks, embeddings, vectorstore)
   - Debe configurar un retriever que busque los 3 fragmentos m√°s relevantes
   - Debe detectar autom√°ticamente qu√© modelo de OpenAI est√° disponible
   - Debe mostrar mensajes informativos durante la configuraci√≥n indicando que es un sistema h√≠brido

2. EVALUACI√ìN DE RELEVANCIA DE DOCUMENTOS:
   - El sistema debe implementar una funci√≥n que eval√∫e si los documentos recuperados son relevantes para la pregunta del usuario
   - La evaluaci√≥n debe considerar:
     - Si los documentos est√°n vac√≠os o no existen ‚Üí No relevante
     - Si el contenido total es menor a 50 caracteres ‚Üí No relevante
     - Si el contenido total es mayor a 200 caracteres ‚Üí Generalmente relevante (confiar en el retriever)
     - Si hay coincidencias de palabras clave importantes (palabras de m√°s de 3 caracteres) entre la pregunta y el contenido ‚Üí Relevante
     - Si el contenido es entre 50-200 caracteres y hay coincidencias de palabras clave (palabras de m√°s de 2 caracteres) ‚Üí Relevante
   - Debe filtrar stopwords en espa√±ol antes de evaluar coincidencias
   - Debe retornar True si hay informaci√≥n relevante, False si no

3. ESTRATEGIA DE RESPUESTA H√çBRIDA:
   - El sistema debe implementar la siguiente estrategia de decisi√≥n:
     a) DETECCI√ìN DE SOLICITUD EXPL√çCITA: Si el usuario pide expl√≠citamente usar conocimiento fuera de las fuentes (palabras clave: 'fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n', 'conocimiento propio', 'entrenamiento'), debe usar directamente el conocimiento del modelo sin buscar en documentaci√≥n
     b) B√öSQUEDA EN DOCUMENTACI√ìN: Si no hay solicitud expl√≠cita, debe buscar documentos relevantes usando el retriever
     c) EVALUACI√ìN DE RELEVANCIA: Debe evaluar si los documentos encontrados son relevantes usando la funci√≥n de evaluaci√≥n
     d) RESPUESTA CON RAG: Si hay informaci√≥n relevante, debe responder usando RAG con la documentaci√≥n
     e) FALLBACK RAG DIRECTO: Si RAG responde con informaci√≥n insuficiente (menos de 50 caracteres y dice "no tengo informaci√≥n" o "no s√©"), debe intentar una vez m√°s con un prompt m√°s directo usando los mismos documentos
     f) RESPUESTA CON CONOCIMIENTO PROPIO: Si no hay informaci√≥n relevante en la documentaci√≥n, debe responder usando el conocimiento del entrenamiento del modelo
   - El sistema debe retornar tanto la respuesta como la fuente utilizada ("documentaci√≥n" o "conocimiento del modelo")

4. M√öLTIPLES MODOS DE RESPUESTA:
   - El sistema debe implementar tres funciones de respuesta:
     a) responder_con_rag: Usa RAG cuando hay informaci√≥n en la documentaci√≥n, con prompt estricto que instruye usar SOLO el contexto proporcionado
     b) responder_con_rag_directo: Usa RAG con documentos ya recuperados como fallback cuando el prompt normal falla, con un prompt m√°s directo y simple
     c) responder_con_conocimiento_propio: Usa el conocimiento del entrenamiento del modelo cuando no hay informaci√≥n relevante en la documentaci√≥n, con prompt que instruye ser honesto y decir "No s√©" cuando no sabe

5. INDICADORES VISUALES DE FUENTE:
   - El sistema debe mostrar claramente qu√© fuente se utiliz√≥ para responder cada pregunta
   - Debe usar emojis diferentes: üìö para documentaci√≥n, üß† para conocimiento propio
   - Debe mostrar el texto "DOCUMENTACI√ìN" o "CONOCIMIENTO PROPIO" junto al emoji
   - Si us√≥ documentaci√≥n, debe mostrar cu√°ntos fragmentos se consultaron

6. CHAT INTERACTIVO:
   - El sistema debe proporcionar una interfaz de chat interactiva por terminal
   - Debe permitir al usuario hacer preguntas de forma continua hasta que decida salir
   - Debe reconocer comandos de salida: 'salir', 'exit', 'quit' (case-insensitive)
   - Debe mostrar mensajes informativos sobre la estrategia h√≠brida al inicio

7. GESTI√ìN DE VARIABLES DE ENTORNO:
   - El sistema debe cargar la API Key de OpenAI desde un archivo .env
   - Debe validar que la API Key existe antes de iniciar el sistema
   - Debe mostrar mensajes de error claros si falta la configuraci√≥n

8. MANEJO DE ERRORES:
   - El sistema debe manejar errores de configuraci√≥n, carga de documento, y consultas al modelo
   - Debe mostrar mensajes de error amigables con sugerencias de soluci√≥n
   - Debe permitir continuar el chat despu√©s de un error

REQUISITOS T√âCNICOS:

1. CONFIGURACI√ìN DE TOKENIZERS:
   - El sistema debe configurar la variable de entorno TOKENIZERS_PARALLELISM="false" antes de importar cualquier m√≥dulo de HuggingFace
   - Esto evita warnings de paralelismo despu√©s de fork

2. IMPORTS REQUERIDOS:
   - Mismos imports que main.py: os, dotenv, langchain_openai, langchain_huggingface (CR√çTICO: NO usar langchain_community.embeddings), langchain_community.document_loaders, langchain_community.vectorstores, langchain_text_splitters, langchain_core.prompts, langchain_core.runnables, langchain_core.output_parsers

3. CONSTANTES DE CONFIGURACI√ìN:
   - DOCUMENTO: "documentacion_tecnica.md"
   - CHROMA_DB_DIR: "./chroma_db"

4. CONFIGURACI√ìN RAG (igual que main.py):
   - Tama√±o de chunk: 500 caracteres
   - Solapamiento: 50 caracteres
   - Modelo de embeddings: sentence-transformers/all-MiniLM-L6-v2
   - Dispositivo: CPU
   - Retriever: busca 3 fragmentos m√°s relevantes

5. CONFIGURACI√ìN DEL MODELO LLM:
   - Temperature: 0.1 (muy baja para reducir alucinaciones)
   - max_tokens: 500
   - Modelo: Detectado autom√°ticamente

6. EVALUACI√ìN DE RELEVANCIA - STOPWORDS EN ESPA√ëOL:
   - El sistema debe filtrar estos stopwords exactos antes de evaluar coincidencias: 'qu√©', 'c√≥mo', 'cu√°ndo', 'd√≥nde', 'por', 'para', 'con', 'de', 'la', 'el', 'un', 'una', 'es', 'son', 'est√°', 'est√°n', 'sobre', 'los', 'las', 'y', 'o', 'pero', 'si', 'no', 'en', 'a', 'que', 'se', 'le', 'te', 'me', 'nos', 'les', 'puedes', 'puede', 'puedo', 'pueden', 'pueda', 'puedan', 'sirve', 'sirven', 'consultar', 'fuera', 'fuentes', 'tus', 'sus', 'mis', 'nuestros', 'vuestros', 'trata', 'como', 'instala', 'instalar'

7. PROMPT TEMPLATES:
   - Template RAG: Debe instruir usar √öNICAMENTE la documentaci√≥n proporcionada, ser exhaustivo si hay informaci√≥n, y decir "No tengo informaci√≥n sobre esto en la documentaci√≥n" si no hay informaci√≥n
   - Template RAG Directo: Debe ser m√°s simple y directo, instruyendo usar SOLO la informaci√≥n de la documentaci√≥n
   - Template Conocimiento Propio: Debe instruir usar conocimiento general, ser honesto, y decir "No s√© sobre..." cuando no sabe

8. DETECCI√ìN DE SOLICITUD EXPL√çCITA:
   - El sistema debe detectar si el usuario pide expl√≠citamente usar conocimiento fuera de fuentes
   - Palabras clave a detectar: 'fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n', 'conocimiento propio', 'entrenamiento'
   - Si detecta estas palabras, debe limpiar la pregunta removiendo estas palabras y usar conocimiento propio directamente

9. API MODERNA DE LANGCHAIN:
   - CR√çTICO: Usar langchain_huggingface para HuggingFaceEmbeddings (NO langchain_community.embeddings)
   - CR√çTICO: Usar retriever.invoke() para buscar documentos (NO m√©todos deprecados)
   - CR√çTICO: Usar LangChain Expression Language (LCEL) con operador pipe para construir cadenas

10. MENSAJES DE USUARIO:
    - Todos los mensajes deben incluir emojis apropiados
    - Debe mostrar claramente qu√© fuente se us√≥ (üìö DOCUMENTACI√ìN o üß† CONOCIMIENTO PROPIO)
    - Debe incluir separadores visuales entre conversaciones
    - Mensajes deben estar en espa√±ol

11. DOCUMENTACI√ìN:
    - Todas las funciones deben tener docstrings en espa√±ol explicando su prop√≥sito
    - Comentarios en espa√±ol cuando sean necesarios
    - C√≥digo debe seguir convenciones PEP 8

RESULTADO ESPERADO:
El script debe demostrar c√≥mo combinar lo mejor de ambos mundos: datos privados mediante RAG y conocimiento general del modelo. El sistema debe decidir inteligentemente qu√© fuente usar para responder cada pregunta, mostrando claramente al usuario si la respuesta viene de la documentaci√≥n o del conocimiento propio del modelo. El usuario debe poder hacer preguntas interactivamente y observar c√≥mo el sistema eval√∫a la relevancia y selecciona la mejor fuente de informaci√≥n.
```

### ‚úÖ Caracter√≠sticas Clave del Prompt

- **Define estrategia completa**: flujo de decisi√≥n paso a paso
- **Especifica m√∫ltiples funciones**: cada una con prop√≥sito claro
- **Detalla l√≥gica de evaluaci√≥n**: c√≥mo determinar relevancia
- **Incluye casos especiales**: detecci√≥n de solicitud expl√≠cita, fallback
- **Define estructura completa**: todas las funciones necesarias

---

## üéì Mejores Pr√°cticas de Prompt Engineering

### 1. **Estructura Clara**
- Separar en secciones: REQUISITOS FUNCIONALES, REQUISITOS T√âCNICOS, ESTRUCTURA ESPERADA
- Usar listas numeradas o con vi√±etas para mejor legibilidad

### 2. **Especificidad**
- Incluir valores exactos: `temperature=0.1`, `chunk_size=500`
- Mencionar librer√≠as espec√≠ficas: `langchain_openai`, `HuggingFaceEmbeddings`
- Definir nombres de funciones y variables esperadas

### 3. **Contexto y Objetivo**
- Empezar con el objetivo educativo o funcional
- Explicar el "por qu√©" adem√°s del "qu√©"

### 4. **Descripciones Detalladas Sin C√≥digo**
- Describir exactamente qu√© debe hacer cada funci√≥n sin mostrar c√≥digo
- Especificar valores exactos, par√°metros, y estructuras de datos esperadas
- Usar lenguaje descriptivo que permita generar c√≥digo preciso

### 5. **Restricciones y Validaciones**
- Especificar qu√© NO debe hacer (evitar alucinaciones)
- Definir manejo de errores esperado

### 6. **Documentaci√≥n Esperada**
- Solicitar docstrings en espa√±ol
- Pedir mensajes informativos para UX

---

## üìä Comparaci√≥n de Prompts

| Aspecto | Script Sin Contexto | Script con RAG | Script H√≠brido |
|---------|-------------------|----------------|----------------|
| **Complejidad** | Baja | Media | Alta |
| **Librer√≠as** | 2 (openai, dotenv) | 7+ (LangChain completo) | 7+ (LangChain completo) |
| **Funciones** | 2 | 3 | 8+ |
| **L√≥gica de Decisi√≥n** | Ninguna | Simple (solo RAG) | Compleja (RAG + evaluaci√≥n + fallback) |
| **Prompt Length** | ~300 palabras | ~500 palabras | ~800 palabras |

---

## üöÄ Uso de los Prompts

### Opci√≥n 1: Copiar y Pegar Directo
Copia el prompt completo en tu herramienta de IA favorita (Claude, GPT-4, etc.) y solicita la generaci√≥n del c√≥digo.

### Opci√≥n 2: Adaptaci√≥n Incremental
1. Empieza con el prompt base
2. Agrega requisitos espec√≠ficos de tu proyecto
3. Refina seg√∫n necesidades

### Opci√≥n 3: Prompt Modular
Divide el prompt en secciones y genera cada parte por separado, luego combina.

---

## ‚ö†Ô∏è Errores Comunes y C√≥mo Evitarlos

### 1. **Error: `'VectorStoreRetriever' object has no attribute 'get_relevant_documents'`**

**Causa**: Uso de m√©todos deprecados de LangChain.

**Soluci√≥n en el prompt**:
- Especificar claramente en el prompt que se debe usar el m√©todo invoke del retriever pasando la pregunta como argumento para buscar documentos
- NO mencionar en el prompt m√©todos como get_relevant_documents o retrieve que est√°n deprecados
- Especificar que en LCEL, el retriever se usa directamente con el operador pipe, donde el operador pipe ejecuta autom√°ticamente invoke
- Describir que para obtener fragmentos consultados se debe llamar al m√©todo invoke del retriever pasando la pregunta, lo cual retorna una lista de documentos

### 2. **Warning: `HuggingFaceEmbeddings` was deprecated**

**Causa**: Uso de import deprecado desde `langchain_community.embeddings`.

**Soluci√≥n en el prompt**:
- Especificar expl√≠citamente en el prompt que se debe importar HuggingFaceEmbeddings desde el paquete langchain_huggingface
- NO mencionar en el prompt importar desde langchain_community.embeddings que est√° deprecado
- Incluir en las dependencias la librer√≠a langchain-huggingface versi√≥n 1.0.0 o superior
- Agregar una nota cr√≠tica en el prompt indicando que usar langchain_community.embeddings est√° deprecado y causar√° warnings

### 3. **Dependencia faltante: `langchain-huggingface`**

**Soluci√≥n en el prompt**:
- Incluir en la lista de dependencias: "langchain-huggingface (requerida para HuggingFaceEmbeddings)"
- Mencionar que debe instalarse: `poetry add langchain-huggingface` o `pip install langchain-huggingface`

### 4. **Timeout al descargar modelos de HuggingFace**

**Soluci√≥n en el prompt**:
- Mencionar que la primera descarga puede tomar tiempo
- Sugerir manejo de errores con mensajes informativos
- Opcional: mencionar que el modelo se cachea localmente despu√©s de la primera descarga

---

## üí° Tips Adicionales

1. **Iteraci√≥n**: Los prompts pueden necesitar refinamiento. Prueba y ajusta.
2. **Especificidad**: Mientras m√°s espec√≠fico, mejor resultado. Incluye valores exactos.
3. **Ejemplos**: Si tienes c√≥digo de referencia, incl√∫yelo en el prompt.
4. **Validaci√≥n**: Siempre prueba el c√≥digo generado antes de usarlo en producci√≥n.
5. **Documentaci√≥n**: Solicita expl√≠citamente documentaci√≥n en espa√±ol si la necesitas.
6. **API Moderna**: Siempre especifica usar la API moderna de LangChain (v1.0+), NO m√©todos deprecados.
7. **Dependencias**: Lista todas las dependencias necesarias, incluyendo `langchain-huggingface`.

---

## üì¶ Dependencias Requeridas

Para que los scripts generados funcionen correctamente, aseg√∫rate de tener estas dependencias en `pyproject.toml`:

```toml
[tool.poetry.dependencies]
python = "^3.13"
langchain = "^1.0.7"
langchain-openai = "^1.0.3"
langchain-community = "^0.4.1"
langchain-huggingface = "^1.0.0"  # ‚ö†Ô∏è REQUERIDA para HuggingFaceEmbeddings
chromadb = "^1.3.4"
python-dotenv = "^1.2.1"
openai = "^2.8.0"
sentence-transformers = "^5.1.2"
```

**Instalaci√≥n**:
```bash
poetry add langchain-huggingface
# o
pip install langchain-huggingface
```

---

## üìù Notas Finales

Estos prompts est√°n dise√±ados para generar c√≥digo funcional y bien documentado. Sin embargo, siempre:

- ‚úÖ Revisa el c√≥digo generado
- ‚úÖ Prueba la funcionalidad
- ‚úÖ Ajusta seg√∫n tus necesidades espec√≠ficas
- ‚úÖ Valida dependencias y configuraciones
- ‚úÖ Verifica que uses la API moderna de LangChain (v1.0+)
- ‚úÖ Aseg√∫rate de tener `langchain-huggingface` instalado

Los prompts pueden adaptarse para otros frameworks o lenguajes cambiando las librer√≠as y estructuras mencionadas.

