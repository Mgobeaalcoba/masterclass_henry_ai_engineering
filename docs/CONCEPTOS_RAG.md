# ğŸ“š Conceptos Fundamentales de RAG - GuÃ­a para Masterclass

Este documento explica los conceptos clave de RAG (Retrieval Augmented Generation) de manera clara y didÃ¡ctica.

---

## ğŸ“‹ Ãndice

1. [Chunks (Fragmentos)](#1-chunks-fragmentos)
2. [Embeddings (Vectores)](#2-embeddings-vectores)
3. [ChromaDB (Base de Datos Vectorial)](#3-chromadb-base-de-datos-vectorial)
4. [Retriever (Recuperador)](#4-retriever-recuperador)
5. [Cadena RAG y LCEL](#5-cadena-rag-y-lcel)

---

## 1. Chunks (Fragmentos)

### Â¿QuÃ© son los Chunks?

Los **chunks** (fragmentos) son **porciones pequeÃ±as de texto** en las que se divide un documento grande para facilitar su procesamiento y recuperaciÃ³n.

### Â¿CuÃ¡l es su uso?

**Problema sin chunks:**
- Un documento de 10,000 palabras es difÃ­cil de procesar
- Buscar informaciÃ³n especÃ­fica requiere leer todo el documento
- Los modelos tienen lÃ­mites de tokens (contexto limitado)

**SoluciÃ³n con chunks:**
- Dividir el documento en fragmentos de 500-1000 palabras
- Cada chunk puede procesarse independientemente
- Buscar solo los chunks relevantes para una pregunta especÃ­fica
- Enviar solo los chunks relevantes al modelo (ahorra tokens y costos)

### Â¿CÃ³mo se configuran?

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # TamaÃ±o mÃ¡ximo de cada chunk (en caracteres)
    chunk_overlap=50,      # Caracteres que se solapan entre chunks
    length_function=len    # FunciÃ³n para medir longitud
)

chunks = text_splitter.split_documents(documentos)
```

**ParÃ¡metros importantes:**

| ParÃ¡metro | DescripciÃ³n | Ejemplo | Efecto |
|-----------|-------------|---------|--------|
| `chunk_size` | TamaÃ±o mÃ¡ximo del fragmento | 500 caracteres | Chunks mÃ¡s grandes = mÃ¡s contexto, pero menos precisiÃ³n |
| `chunk_overlap` | Solapamiento entre chunks | 50 caracteres | Evita perder informaciÃ³n en los bordes |
| `length_function` | CÃ³mo medir longitud | `len` | Puede usar tokens, palabras, etc. |

**Â¿Por quÃ© overlap (solapamiento)?**

```
Documento original:
"HenryPy es una librerÃ­a de Python. Para instalarla usa: pip install henrypy"

Sin overlap:
Chunk 1: "HenryPy es una librerÃ­a de Python."
Chunk 2: "Para instalarla usa: pip install henrypy"

Con overlap (50 chars):
Chunk 1: "HenryPy es una librerÃ­a de Python. Para instalarla"
Chunk 2: "una librerÃ­a de Python. Para instalarla usa: pip install henrypy"
```

El overlap asegura que informaciÃ³n importante en los bordes no se pierda.

---

## 2. Embeddings (Vectores)

### Â¿QuÃ© son los Embeddings?

Los **embeddings** son **representaciones numÃ©ricas (vectores) de texto** que capturan el significado semÃ¡ntico. Son arrays de nÃºmeros que representan el "sentido" del texto.

### Â¿CuÃ¡l es su uso?

**Problema:**
- Las computadoras no entienden texto directamente
- Necesitamos una forma de comparar textos por significado, no por palabras exactas

**SoluciÃ³n con embeddings:**
- Convertir texto â†’ vector de nÃºmeros (ej: [0.2, -0.5, 0.8, ...])
- Textos similares tienen vectores similares
- Podemos buscar textos similares usando matemÃ¡ticas (distancia entre vectores)

**Ejemplo visual:**

```
"Instalar Python"     â†’ [0.2, -0.3, 0.5, 0.1, ...]
"Instalar librerÃ­a"    â†’ [0.25, -0.28, 0.48, 0.12, ...]  â† Similar (cerca en espacio)
"Comer pizza"          â†’ [-0.8, 0.4, -0.2, 0.9, ...]      â† Diferente (lejos en espacio)
```

### Â¿CÃ³mo se configuran?

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'}  # o 'cuda' para GPU
)

# Convertir texto a vector
vector = embeddings.embed_query("Â¿CÃ³mo instalo HenryPy?")
# Resultado: [0.123, -0.456, 0.789, ...] (vector de 384 nÃºmeros)
```

**Modelos comunes:**

| Modelo | TamaÃ±o | Dimensiones | Uso |
|--------|--------|-------------|-----|
| `all-MiniLM-L6-v2` | ~80MB | 384 | RÃ¡pido, bueno para general |
| `all-mpnet-base-v2` | ~420MB | 768 | MÃ¡s preciso, mÃ¡s lento |
| `text-embedding-ada-002` (OpenAI) | API | 1536 | Muy preciso, requiere API key |

**En nuestro proyecto:**
- Usamos `all-MiniLM-L6-v2` porque es **gratis** y funciona localmente
- Genera vectores de 384 dimensiones
- No requiere conexiÃ³n a internet despuÃ©s de la primera descarga

### Â¿QuÃ© relaciÃ³n tienen con los Chunks?

**Flujo completo:**

```
1. Documento original
   â†“
2. Dividir en CHUNKS (fragmentos de texto)
   â†“
3. Convertir cada CHUNK a EMBEDDING (vector numÃ©rico)
   â†“
4. Almacenar embeddings en base vectorial (ChromaDB)
   â†“
5. Cuando llega una pregunta:
   - Convertir pregunta a embedding
   - Buscar chunks con embeddings similares
   - Recuperar los chunks mÃ¡s relevantes
```

**Ejemplo prÃ¡ctico:**

```python
# 1. Documento original
documento = "HenryPy es una librerÃ­a de Python..."

# 2. Dividir en chunks
chunks = [
    "HenryPy es una librerÃ­a de Python diseÃ±ada para...",
    "Para instalar HenryPy usa: pip install henrypy...",
    "La configuraciÃ³n requiere una API_KEY..."
]

# 3. Convertir cada chunk a embedding
embeddings_chunk1 = [0.1, -0.2, 0.3, ...]  # Vector de 384 nÃºmeros
embeddings_chunk2 = [0.15, -0.18, 0.28, ...]
embeddings_chunk3 = [0.05, -0.25, 0.35, ...]

# 4. Pregunta del usuario
pregunta = "Â¿CÃ³mo instalo HenryPy?"
embedding_pregunta = [0.14, -0.19, 0.29, ...]  # Similar a chunk2

# 5. Buscar chunk mÃ¡s similar (usando distancia matemÃ¡tica)
# Resultado: chunk2 es el mÃ¡s relevante
```

---

## 3. ChromaDB (Base de Datos Vectorial)

### Â¿QuÃ© es ChromaDB?

**ChromaDB** es una **base de datos especializada** para almacenar y buscar **vectores (embeddings)** de manera eficiente.

### Â¿QuÃ© tiene de particular?

**CaracterÃ­sticas principales:**

1. **BÃºsqueda por similitud semÃ¡ntica**: Encuentra textos similares, no exactos
2. **Optimizada para vectores**: DiseÃ±ada especÃ­ficamente para operaciones con embeddings
3. **BÃºsqueda rÃ¡pida**: Usa algoritmos especializados (como LSH, HNSW) para bÃºsqueda rÃ¡pida
4. **Persistencia**: Guarda los datos en disco (no se pierden al cerrar)
5. **Ligera**: No requiere servidor, funciona como biblioteca Python

### Â¿Por quÃ© no usar SQLite para la misma tarea?

**SQLite vs ChromaDB:**

| Aspecto | SQLite | ChromaDB |
|---------|--------|----------|
| **Tipo de bÃºsqueda** | Exacta (WHERE texto = "X") | Por similitud semÃ¡ntica |
| **OptimizaciÃ³n** | Para texto exacto | Para vectores numÃ©ricos |
| **BÃºsqueda semÃ¡ntica** | âŒ No nativa | âœ… Nativa y rÃ¡pida |
| **Ejemplo** | Buscar "instalar" encuentra solo "instalar" | Buscar "instalar" encuentra "instalaciÃ³n", "setup", "configurar" |

**Ejemplo prÃ¡ctico:**

**Con SQLite:**
```sql
-- Solo encuentra coincidencias exactas
SELECT * FROM documentos WHERE texto LIKE '%instalar%';
-- No encuentra: "instalaciÃ³n", "setup", "configurar"
```

**Con ChromaDB:**
```python
# Encuentra textos semÃ¡nticamente similares
results = vectorstore.similarity_search("instalar")
# Encuentra: "instalaciÃ³n", "setup", "configurar", "instalar"
```

**Â¿CuÃ¡ndo usar cada uno?**

- **SQLite**: Datos estructurados, bÃºsquedas exactas, relaciones complejas
- **ChromaDB**: BÃºsqueda semÃ¡ntica, RAG, recomendaciones, bÃºsqueda por significado

**En nuestro proyecto:**
- Usamos ChromaDB porque necesitamos **bÃºsqueda semÃ¡ntica**
- Cuando preguntamos "instalar HenryPy", queremos encontrar chunks sobre "instalaciÃ³n", "setup", etc.
- SQLite no puede hacer esto eficientemente

---

## 4. Retriever (Recuperador)

### Â¿QuÃ© es un Retriever?

Un **retriever** es un componente que **busca y recupera los chunks mÃ¡s relevantes** de la base vectorial para una pregunta especÃ­fica.

### Â¿CuÃ¡l es su utilidad en un RAG?

**FunciÃ³n principal:**
1. Recibe una pregunta del usuario
2. Convierte la pregunta a embedding
3. Busca en ChromaDB los chunks con embeddings mÃ¡s similares
4. Retorna los N chunks mÃ¡s relevantes

**Sin retriever:**
- TendrÃ­amos que buscar manualmente en todos los chunks
- No sabrÃ­amos cuÃ¡les son relevantes
- EnviarÃ­amos informaciÃ³n irrelevante al modelo (desperdicio de tokens)

**Con retriever:**
- Encuentra automÃ¡ticamente los chunks relevantes
- Solo envÃ­a informaciÃ³n Ãºtil al modelo
- Ahorra tokens y mejora la precisiÃ³n

### Â¿QuÃ© son los fragmentos?

Los **fragmentos** son los **chunks recuperados** por el retriever. Son las porciones de texto que el retriever considera mÃ¡s relevantes para responder la pregunta.

**Ejemplo:**

```python
# Pregunta del usuario
pregunta = "Â¿CÃ³mo instalo HenryPy?"

# Retriever busca y encuentra los 3 fragmentos mÃ¡s relevantes
fragmentos = retriever.invoke(pregunta)
# Resultado:
# [
#   "Para instalar HenryPy, usa: pip install henrypy...",
#   "La instalaciÃ³n estÃ¡ndar se realiza a travÃ©s de pip...",
#   "Si olvidas este paso, las funciones fallarÃ¡n..."
# ]

# Estos fragmentos se envÃ­an al modelo como contexto
```

**ConfiguraciÃ³n del retriever:**

```python
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # Retorna los 3 fragmentos mÃ¡s relevantes
)
```

**ParÃ¡metro `k`:**
- `k=3`: Retorna los 3 fragmentos mÃ¡s relevantes
- `k=5`: Retorna los 5 fragmentos mÃ¡s relevantes
- MÃ¡s fragmentos = mÃ¡s contexto, pero tambiÃ©n mÃ¡s tokens y posiblemente informaciÃ³n menos relevante

---

## 5. Cadena RAG y LCEL

### Â¿QuÃ© es una Cadena RAG?

Una **cadena RAG** es el **flujo completo** que conecta todos los componentes: pregunta â†’ bÃºsqueda â†’ contexto â†’ respuesta.

**Componentes de la cadena:**

```
1. PREGUNTA del usuario
   â†“
2. RETRIEVER busca chunks relevantes
   â†“
3. FORMAT_DOCS formatea los chunks
   â†“
4. PROMPT template combina contexto + pregunta
   â†“
5. LLM genera respuesta usando el contexto
   â†“
6. OUTPUT_PARSER formatea la respuesta
   â†“
7. RESPUESTA final
```

### Â¿QuÃ© es LCEL?

**LCEL** (LangChain Expression Language) es una forma **declarativa y funcional** de construir cadenas en LangChain usando el operador `|` (pipe).

**Sintaxis tradicional (sin LCEL):**
```python
# Complicado y verboso
docs = retriever.invoke(pregunta)
formatted_docs = format_docs(docs)
prompt_input = {"context": formatted_docs, "question": pregunta}
prompt_output = prompt.invoke(prompt_input)
llm_output = llm.invoke(prompt_output)
respuesta = output_parser.invoke(llm_output)
```

**Sintaxis LCEL (moderna):**
```python
# Simple y elegante
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

respuesta = rag_chain.invoke(pregunta)
```

### Â¿CÃ³mo funciona LCEL?

**Operador `|` (pipe):**
- Similar a pipes en terminal: `cat file.txt | grep "texto" | head -5`
- Cada componente se ejecuta secuencialmente
- La salida de uno es la entrada del siguiente

**Componentes LCEL:**

```python
rag_chain = (
    # PASO 1: Preparar inputs
    {
        "context": retriever | format_docs,  # Buscar y formatear chunks
        "question": RunnablePassthrough()     # Pasar pregunta sin modificar
    }
    |  # â†“
    # PASO 2: Crear prompt
    prompt  # Combina contexto + pregunta en un prompt
    |  # â†“
    # PASO 3: Generar respuesta
    llm  # Modelo de lenguaje genera respuesta
    |  # â†“
    # PASO 4: Formatear salida
    StrOutputParser()  # Convierte a string simple
)
```

**Ventajas de LCEL:**

1. **Legible**: Se lee de izquierda a derecha como un flujo
2. **Composable**: FÃ¡cil agregar/quitar componentes
3. **Eficiente**: LangChain optimiza la ejecuciÃ³n
4. **Moderno**: Es la forma recomendada en LangChain v1.0+

**Ejemplo paso a paso:**

```python
# Input
pregunta = "Â¿CÃ³mo instalo HenryPy?"

# Paso 1: Retriever busca chunks
chunks = retriever.invoke(pregunta)
# Resultado: [chunk1, chunk2, chunk3]

# Paso 2: Formatear chunks
contexto = format_docs(chunks)
# Resultado: "chunk1\n\nchunk2\n\nchunk3"

# Paso 3: Crear prompt
prompt_completo = prompt.format(context=contexto, question=pregunta)
# Resultado: "Eres un asistente...\nContexto: chunk1...\nPregunta: Â¿CÃ³mo instalo HenryPy?"

# Paso 4: LLM genera respuesta
respuesta_llm = llm.invoke(prompt_completo)
# Resultado: Objeto Message con contenido

# Paso 5: Parsear a string
respuesta_final = StrOutputParser().invoke(respuesta_llm)
# Resultado: "Para instalar HenryPy, usa: pip install henrypy..."
```

**Con LCEL, todo esto se hace automÃ¡ticamente:**

```python
respuesta = rag_chain.invoke(pregunta)
# Â¡Listo! Todo el flujo se ejecuta automÃ¡ticamente
```

---

## ğŸ”„ Flujo Completo de RAG (Resumen Visual)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENTO ORIGINAL                        â”‚
â”‚         "documentacion_tecnica.md" (texto plano)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   TEXT SPLITTER (Chunks)      â”‚
        â”‚   Divide en fragmentos de 500  â”‚
        â”‚   caracteres con overlap 50   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   EMBEDDINGS                  â”‚
        â”‚   Convierte cada chunk a      â”‚
        â”‚   vector numÃ©rico (384 dim)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   CHROMADB                    â”‚
        â”‚   Almacena vectores para      â”‚
        â”‚   bÃºsqueda rÃ¡pida             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“ (Cuando llega pregunta)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   RETRIEVER                   â”‚
        â”‚   Busca chunks mÃ¡s relevantes â”‚
        â”‚   (k=3 fragmentos)           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   CADENA RAG (LCEL)           â”‚
        â”‚   retriever â†’ format â†’ prompt â”‚
        â”‚   â†’ llm â†’ parser              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   RESPUESTA FINAL             â”‚
        â”‚   Con contexto de documentaciÃ³nâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Resumen de Conceptos Clave

| Concepto | Â¿QuÃ© es? | Â¿Para quÃ© sirve? | Ejemplo |
|----------|----------|------------------|---------|
| **Chunk** | Fragmento de texto | Dividir documentos grandes | "Para instalar HenryPy usa pip..." |
| **Embedding** | Vector numÃ©rico | Representar significado del texto | [0.2, -0.3, 0.5, ...] |
| **ChromaDB** | Base de datos vectorial | Buscar por similitud semÃ¡ntica | Encuentra textos similares |
| **Retriever** | Componente de bÃºsqueda | Encontrar chunks relevantes | Retorna los 3 mejores chunks |
| **Cadena RAG** | Flujo completo | Conectar todos los componentes | pregunta â†’ bÃºsqueda â†’ respuesta |
| **LCEL** | Lenguaje de expresiones | Construir cadenas de forma elegante | `retriever \| prompt \| llm` |

---

## ğŸ¯ Preguntas Frecuentes

### Â¿Por quÃ© dividir en chunks si puedo enviar todo el documento?

- **LÃ­mites de tokens**: Los modelos tienen lÃ­mites (ej: 4K, 8K, 32K tokens)
- **Costo**: MÃ¡s tokens = mÃ¡s costo
- **PrecisiÃ³n**: Solo enviar informaciÃ³n relevante mejora la respuesta
- **Velocidad**: Procesar chunks pequeÃ±os es mÃ¡s rÃ¡pido

### Â¿QuÃ© tamaÃ±o de chunk es mejor?

- **Chunks pequeÃ±os (200-300)**: MÃ¡s precisos, pero pueden perder contexto
- **Chunks medianos (500-800)**: Balance entre precisiÃ³n y contexto (recomendado)
- **Chunks grandes (1000+)**: MÃ¡s contexto, pero menos precisiÃ³n

**RecomendaciÃ³n**: Empieza con 500 y ajusta segÃºn resultados.

### Â¿Por quÃ© usar embeddings locales (HuggingFace) vs API (OpenAI)?

| Aspecto | HuggingFace (Local) | OpenAI (API) |
|---------|---------------------|--------------|
| **Costo** | âœ… Gratis | âŒ Pago por uso |
| **Velocidad** | âš ï¸ Primera vez lenta (descarga) | âœ… RÃ¡pido |
| **Privacidad** | âœ… 100% local | âš ï¸ EnvÃ­a datos a API |
| **PrecisiÃ³n** | âš ï¸ Buena | âœ… Excelente |

**RecomendaciÃ³n**: Para desarrollo y demos, usa HuggingFace. Para producciÃ³n, considera OpenAI.

### Â¿CuÃ¡ntos fragmentos (k) debo recuperar?

- **k=1**: Muy especÃ­fico, puede perder contexto
- **k=3**: Balance recomendado (usado en nuestro proyecto)
- **k=5**: MÃ¡s contexto, pero puede incluir informaciÃ³n menos relevante
- **k=10+**: Demasiado contexto, puede confundir al modelo

**RecomendaciÃ³n**: Empieza con k=3 y ajusta segÃºn resultados.

---

## ğŸ“š Recursos Adicionales

- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [HuggingFace Sentence Transformers](https://www.sbert.net/)
- [RAG Paper Original](https://arxiv.org/abs/2005.11401)

---

**Â¡Listo para tu masterclass!** ğŸš€

Este documento cubre todos los conceptos fundamentales que necesitas explicar. Puedes usarlo como guÃ­a durante la presentaciÃ³n o compartirlo con los participantes.

