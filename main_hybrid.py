#!/usr/bin/env python
"""
Demostraci√≥n: LLM H√çBRIDO - RAG + Conocimiento del Modelo

Combina RAG con conocimiento del entrenamiento del modelo:
1. Si est√° en la documentaci√≥n ‚Üí Responde con RAG
2. Si NO est√° en la documentaci√≥n ‚Üí Responde con conocimiento del modelo
3. Si tampoco sabe ‚Üí Dice "No s√©"
"""

import os
# Configurar tokenizers para evitar warnings de paralelismo despu√©s de fork
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# Configuraci√≥n
DOCUMENTO = "documentacion_tecnica.md"
CHROMA_DB_DIR = "./chroma_db"

def detectar_modelo():
    """Detecta qu√© modelo de OpenAI est√° disponible en tu cuenta."""
    for modelo in ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]:
        try:
            ChatOpenAI(model=modelo, temperature=0, max_tokens=5).invoke("test")
            return modelo
        except:
            continue
    return None

def configurar_sistema_hibrido():
    """
    Configura el sistema h√≠brido: RAG + conocimiento del modelo.
    
    Returns:
        tuple: (retriever, llm, modelo_actual, vectorstore)
    """
    print("\nüîß Configurando Sistema H√≠brido (RAG + Conocimiento del Modelo)...\n")
    
    # 1. Cargar documento
    print("üìÑ Cargando documento:", DOCUMENTO)
    documentos = TextLoader(DOCUMENTO, encoding="utf-8").load()
    print(f"   ‚úÖ Documento cargado ({len(documentos)} archivo(s))\n")
    
    # 2. Dividir en fragmentos
    print("‚úÇÔ∏è  Dividiendo documento en fragmentos...")
    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len).split_documents(documentos)
    print(f"   ‚úÖ {len(chunks)} fragmentos creados\n")
    
    # 3. Crear embeddings y base vectorial
    print("üß† Creando embeddings (HuggingFace - 100% GRATIS) y base vectorial...")
    print("   ‚è≥ Primera vez puede tomar un momento (descarga modelo ~400MB)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={'device': 'cpu'})
    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=CHROMA_DB_DIR)
    print("   ‚úÖ Base vectorial creada (sin costo!)\n")
    
    # 4. Detectar y configurar modelo
    print("üîç Detectando modelo disponible...")
    modelo = detectar_modelo()
    if not modelo:
        raise Exception("No se encontr√≥ ning√∫n modelo disponible")
    print(f"   ‚úÖ Usando modelo: {modelo}\n")
    
    # Configuraci√≥n anti-alucinaci√≥n
    llm = ChatOpenAI(model=modelo, temperature=0.1, max_tokens=500)
    
    # 5. Crear retriever (busca los 3 fragmentos m√°s relevantes)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    print("   ‚úÖ Sistema h√≠brido listo\n")
    
    return retriever, llm, modelo, vectorstore

def evaluar_relevancia_documentos(docs, pregunta):
    """
    Eval√∫a si los documentos recuperados son relevantes para la pregunta.
    
    Estrategia mejorada: Verifica si hay coincidencias significativas de palabras clave
    espec√≠ficas del tema. Si encuentra documentos con contenido sustancial, los considera relevantes.
    
    Args:
        docs: Lista de documentos recuperados
        pregunta: Pregunta original del usuario
        
    Returns:
        bool: True si hay informaci√≥n relevante, False si no
    """
    if not docs or len(docs) == 0:
        return False
    
    # Verificar contenido de documentos
    contenido_texto = " ".join(doc.page_content.lower() for doc in docs)
    contenido_total = len(contenido_texto)
    
    # Si el contenido total es muy peque√±o, no es relevante
    if contenido_total < 50:
        return False
    
    # Extraer palabras clave de la pregunta (palabras significativas)
    pregunta_lower = pregunta.lower()
    palabras_pregunta = set(pregunta_lower.split())
    
    # Palabras comunes a ignorar (stopwords en espa√±ol)
    palabras_ignorar = {
        'qu√©', 'c√≥mo', 'cu√°ndo', 'd√≥nde', 'por', 'para', 'con', 'de', 'la', 'el', 'un', 'una',
        'es', 'son', 'est√°', 'est√°n', 'sobre', 'el', 'la', 'los', 'las', 'un', 'una',
        'y', 'o', 'pero', 'si', 'no', 'en', 'a', 'de', 'que', 'se', 'le', 'te', 'me', 'nos', 'les',
        'puedes', 'puede', 'puedo', 'pueden', 'pueda', 'puedan', 'para', 'que', 'sirve', 'sirven',
        'consultar', 'fuera', 'fuentes', 'tus', 'sus', 'mis', 'nuestros', 'vuestros',
        'trata', 'se', 'trata', 'como', 'instala', 'instalar'
    }
    
    palabras_clave = palabras_pregunta - palabras_ignorar
    
    # Si hay contenido sustancial (m√°s de 200 caracteres), considerar relevante por defecto
    # El retriever ya hizo el trabajo de encontrar documentos similares
    if contenido_total > 200:
        # Verificar si hay al menos una palabra clave importante en el contenido
        # Esto evita casos donde el contenido es grande pero no relacionado
        palabras_importantes = [p for p in palabras_clave if len(p) > 3]  # Palabras de m√°s de 3 caracteres
        if len(palabras_importantes) == 0:
            # Si no hay palabras importantes, pero el contenido es sustancial, confiar en el retriever
            return True
        
        # Verificar si alguna palabra importante aparece
        tiene_coincidencias = any(palabra in contenido_texto for palabra in palabras_importantes)
        if tiene_coincidencias:
            return True
    
    # Si el contenido es menor pero hay coincidencias significativas
    if contenido_total >= 50:
        coincidencias = sum(1 for palabra in palabras_clave if len(palabra) > 2 and palabra in contenido_texto)
        if coincidencias > 0:
            return True
    
    return False

def responder_con_rag(pregunta, retriever, llm):
    """
    Responde usando RAG cuando hay informaci√≥n en la documentaci√≥n.
    
    Args:
        pregunta: Pregunta del usuario
        retriever: Retriever de documentos
        llm: Modelo de lenguaje
        
    Returns:
        str: Respuesta generada con RAG
    """
    template_rag = """Eres un asistente √∫til que responde preguntas bas√°ndote √öNICAMENTE en la documentaci√≥n proporcionada.

Contexto de la documentaci√≥n:
{context}

Pregunta del usuario: {question}

INSTRUCCIONES CR√çTICAS:
- DEBES responder usando la informaci√≥n del contexto proporcionado
- Si el contexto contiene informaci√≥n sobre el tema, √∫sala para responder de manera completa
- Extrae TODA la informaci√≥n relevante del contexto y pres√©ntala de forma clara
- NO uses conocimiento fuera del contexto proporcionado
- Si el contexto NO contiene informaci√≥n sobre la pregunta, entonces di "No tengo informaci√≥n sobre esto en la documentaci√≥n"
- S√© exhaustivo: si hay informaci√≥n en el contexto, √∫sala toda"""
    
    prompt_rag = ChatPromptTemplate.from_template(template_rag)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt_rag
        | llm
        | StrOutputParser()
    )
    
    return rag_chain.invoke(pregunta)

def responder_con_rag_directo(pregunta, docs, llm):
    """
    Responde usando RAG con documentos ya recuperados (fallback cuando el prompt normal falla).
    
    Args:
        pregunta: Pregunta del usuario
        docs: Documentos ya recuperados
        llm: Modelo de lenguaje
        
    Returns:
        str: Respuesta generada con RAG
    """
    template_directo = """Responde la siguiente pregunta usando la informaci√≥n proporcionada en la documentaci√≥n.

Documentaci√≥n:
{context}

Pregunta: {question}

Responde usando SOLO la informaci√≥n de la documentaci√≥n. Si hay informaci√≥n relevante, √∫sala para responder completamente."""
    
    prompt_directo = ChatPromptTemplate.from_template(template_directo)
    
    contexto = "\n\n".join(doc.page_content for doc in docs)
    
    # Crear mensaje directamente
    mensaje = prompt_directo.format(context=contexto, question=pregunta)
    respuesta = llm.invoke(mensaje)
    
    return respuesta.content if hasattr(respuesta, 'content') else str(respuesta)

def responder_con_conocimiento_propio(pregunta, llm):
    """
    Responde usando el conocimiento del entrenamiento del modelo.
    
    Args:
        pregunta: Pregunta del usuario
        llm: Modelo de lenguaje
        
    Returns:
        str: Respuesta generada con conocimiento del modelo
    """
    template_propio = """Eres un asistente √∫til y honesto. Responde la pregunta usando tu conocimiento de entrenamiento.

Pregunta: {question}

INSTRUCCIONES:
- Responde usando tu conocimiento general si lo tienes
- Si NO sabes la respuesta, di claramente "No s√© sobre..." o "No tengo informaci√≥n sobre..."
- NO inventes informaci√≥n. S√© honesto y directo."""
    
    prompt_propio = ChatPromptTemplate.from_template(template_propio)
    
    chain_propio = (
        {"question": RunnablePassthrough()}
        | prompt_propio
        | llm
        | StrOutputParser()
    )
    
    return chain_propio.invoke(pregunta)

def responder_hibrido(pregunta, retriever, llm):
    """
    Responde usando estrategia h√≠brida: primero RAG, luego conocimiento propio.
    
    Args:
        pregunta: Pregunta del usuario
        retriever: Retriever de documentos
        llm: Modelo de lenguaje
        
    Returns:
        tuple: (respuesta, fuente_usada)
    """
    # Detectar si el usuario expl√≠citamente pide usar conocimiento fuera de las fuentes
    pregunta_lower = pregunta.lower()
    palabras_fuera = {'fuera', 'fuentes', 'consultar', 'por fuera', 'sin documentaci√≥n', 'conocimiento propio', 'entrenamiento'}
    pedir_fuera = any(palabra in pregunta_lower for palabra in palabras_fuera)
    
    # Si el usuario expl√≠citamente pide usar conocimiento fuera, hacerlo directamente
    if pedir_fuera:
        # Limpiar la pregunta removiendo las palabras de "fuera"
        pregunta_limpia = pregunta
        for palabra in palabras_fuera:
            pregunta_limpia = pregunta_limpia.replace(palabra, "").strip()
        respuesta = responder_con_conocimiento_propio(pregunta_limpia if pregunta_limpia else pregunta, llm)
        return respuesta, "conocimiento del modelo"
    
    # 1. Buscar en documentaci√≥n
    docs = retriever.invoke(pregunta)
    
    # 2. Evaluar si hay informaci√≥n relevante
    if evaluar_relevancia_documentos(docs, pregunta):
        # Usar RAG con documentaci√≥n - confiar en los documentos encontrados
        respuesta = responder_con_rag(pregunta, retriever, llm)
        
        # Si encontramos documentos relevantes, confiar en RAG
        # Solo cambiar a conocimiento propio si la respuesta es muy corta o claramente indica falta de info
        respuesta_lower = respuesta.lower()
        tiene_info_insuficiente = (
            ("no tengo informaci√≥n" in respuesta_lower or "no s√©" in respuesta_lower) 
            and len(respuesta) < 50  # Respuesta muy corta
        )
        
        # Si RAG dice que no tiene info pero encontramos documentos relevantes, 
        # es probable que el prompt no est√© funcionando bien, pero a√∫n as√≠ confiar en RAG
        # porque los documentos S√ç tienen informaci√≥n
        if tiene_info_insuficiente:
            # Intentar una vez m√°s con un prompt m√°s directo
            respuesta_directa = responder_con_rag_directo(pregunta, docs, llm)
            if len(respuesta_directa) > 50:  # Si la respuesta directa tiene contenido
                return respuesta_directa, "documentaci√≥n"
        
        return respuesta, "documentaci√≥n"
    else:
        # Usar conocimiento propio del modelo
        respuesta = responder_con_conocimiento_propio(pregunta, llm)
        return respuesta, "conocimiento del modelo"

def main():
    """
    Funci√≥n principal: Sistema h√≠brido que combina RAG con conocimiento del modelo.
    
    Estrategia:
    1. Busca primero en la documentaci√≥n (RAG)
    2. Si no encuentra informaci√≥n relevante, usa conocimiento del modelo
    3. Si tampoco sabe, el modelo dice "No s√©"
    """
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: Configura OPENAI_API_KEY en el archivo .env")
        return
    
    try:
        retriever, llm, modelo, vectorstore = configurar_sistema_hibrido()
        
        print("="*70)
        print("  üí¨ Chat H√çBRIDO - RAG + Conocimiento del Modelo")
        print("="*70)
        print(f"\nüí° El modelo busca primero en {DOCUMENTO}")
        print("üí° Si no encuentra informaci√≥n, usa su conocimiento de entrenamiento")
        print("üí° Escribe 'salir' para terminar\n")
        print("üéØ Prueba preguntando:")
        print("   - ¬øC√≥mo instalo la librer√≠a HenryPy? (est√° en documentaci√≥n)")
        print("   - ¬øQu√© es Python? (conocimiento general)\n")
        
        while True:
            pregunta = input("üßë T√ö: ").strip()
            
            if pregunta.lower() in ['salir', 'exit', 'quit']:
                print("\nüëã ¬°Hasta luego!\n")
                break
            
            if not pregunta:
                continue
            
            try:
                print(f"\n‚è≥ Analizando pregunta y consultando {modelo}...\n")
                
                # Responder con estrategia h√≠brida
                respuesta, fuente = responder_hibrido(pregunta, retriever, llm)
                
                # Mostrar respuesta con indicador de fuente
                fuente_emoji = "üìö" if fuente == "documentaci√≥n" else "üß†"
                fuente_texto = "DOCUMENTACI√ìN" if fuente == "documentaci√≥n" else "CONOCIMIENTO PROPIO"
                
                print(f"ü§ñ {modelo.upper()} ({fuente_emoji} {fuente_texto}): {respuesta}\n")
                
                # Mostrar documentos consultados si us√≥ RAG
                if fuente == "documentaci√≥n":
                    docs = retriever.invoke(pregunta)
                    print(f"üìö Fuentes: {len(docs)} fragmentos consultados de la documentaci√≥n")
                
                print("-" * 70 + "\n")
                
            except Exception as e:
                print(f"‚ùå Error: {e}\n")
    
    except Exception as e:
        print(f"‚ùå Error al configurar sistema h√≠brido: {e}\n")
        print("üí° Aseg√∫rate de que:")
        print(f"   - El archivo {DOCUMENTO} existe")
        print("   - Tu API Key de OpenAI es v√°lida")
        print("   - Tienes las dependencias instaladas (poetry install)\n")

if __name__ == "__main__":
    main()

